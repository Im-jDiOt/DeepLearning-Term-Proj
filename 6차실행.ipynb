{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Im-jDiOt/DeepLearning-Term-Proj/blob/feature-resnet/6%EC%B0%A8%EC%8B%A4%ED%96%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBYrCzXvBVMH",
        "outputId": "5c266a09-4daf-4800-d0e4-7b76aee4b34a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import timm  # timm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "\n",
        "# --- 1. ë°ì´í„° ë¡œë“œ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° (ìºê¸€ ê²½ë¡œ ìˆ˜ì •) ---\n",
        "\n",
        "# [ìˆ˜ì •] ìºê¸€ ë°ì´í„°ì…‹ ê¸°ë³¸ ê²½ë¡œ\n",
        "base_dir = r'/kaggle/input/state-farm-distracted-driver-detection'\n",
        "\n",
        "# [ìˆ˜ì •] driver_imgs_list.csv íŒŒì¼ ê²½ë¡œ\n",
        "driver_csv_path = os.path.join(base_dir, 'driver_imgs_list.csv')\n",
        "\n",
        "# [ìˆ˜ì •] 'imgs' í´ë” ë‚´ì˜ train/test ê²½ë¡œ\n",
        "train_dir = os.path.join(base_dir, 'imgs', 'train')\n",
        "test_dir = os.path.join(base_dir, 'imgs', 'test')\n",
        "\n",
        "img_size = 224\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "num_epochs = 40\n",
        "learning_rate = 0.001\n",
        "\n",
        "# [ìˆ˜ì •] ìºê¸€ì€ num_workers=2 ë˜ëŠ” 4ê°€ ë” ë¹ ë¦…ë‹ˆë‹¤.\n",
        "num_workers = 2\n",
        "\n",
        "print(f\"Train directory: {train_dir}\")\n",
        "print(f\"Test directory: {test_dir}\")\n",
        "print(f\"ì´ë¯¸ì§€ í¬ê¸°: {img_size}x{img_size}\")\n",
        "print(f\"ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
        "\n",
        "driver_df = pd.read_csv(driver_csv_path)\n",
        "print(f\"ê³ ìœ  ìš´ì „ì ìˆ˜: {driver_df['subject'].nunique()}ëª…\")\n",
        "\n",
        "# --- 2. 5-Fold êµì°¨ ê²€ì¦ (ìš´ì „ì ê¸°ì¤€ ë¶„í• ) ---\n",
        "all_drivers = sorted(driver_df['subject'].unique())\n",
        "n_folds = 5\n",
        "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "fold_splits = []\n",
        "print(\"K-Fold ìš´ì „ì ë¶„í• :\")\n",
        "for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(all_drivers)):\n",
        "    train_drivers = [all_drivers[i] for i in train_indices]\n",
        "    val_drivers = [all_drivers[i] for i in val_indices]\n",
        "\n",
        "    fold_splits.append({\n",
        "        'fold': fold_idx+1,\n",
        "        'train_drivers': train_drivers,\n",
        "        'val_drivers': val_drivers\n",
        "    })\n",
        "\n",
        "    # (ë¡œê·¸ ì¶œë ¥ì€ ê°„ê²°í•˜ê²Œ)\n",
        "    train_imgs = driver_df[driver_df['subject'].isin(train_drivers)]\n",
        "    val_imgs = driver_df[driver_df['subject'].isin(val_drivers)]\n",
        "    print(f\"  Fold {fold_idx+1}: í•™ìŠµ {len(train_imgs)}ê°œ | ê²€ì¦ {len(val_imgs)}ê°œ\")\n",
        "\n",
        "\n",
        "# --- 3. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ (DriverDataset) [ë¸”ë™ë¦¬ìŠ¤íŠ¸ ê¸°ëŠ¥ ì¶”ê°€] ---\n",
        "\n",
        "class DriverDataset(Dataset):\n",
        "    \"\"\"ìš´ì „ì í–‰ë™ ë°ì´í„°ì…‹ (ë¸”ë™ë¦¬ìŠ¤íŠ¸ ê¸°ëŠ¥ ì¶”ê°€)\"\"\"\n",
        "\n",
        "    # [ìˆ˜ì •] blacklist_files íŒŒë¼ë¯¸í„° ì¶”ê°€\n",
        "    def __init__(self, data_dir, driver_df, driver_list, transform=None, is_test=False,\n",
        "                 blacklist_files=None):\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        # [ìˆ˜ì •] ë¸”ë™ë¦¬ìŠ¤íŠ¸ë¥¼ setìœ¼ë¡œ ë³€ê²½ (ê²€ìƒ‰ ì†ë„ í–¥ìƒ)\n",
        "        self.blacklist = set(blacklist_files) if blacklist_files else set()\n",
        "\n",
        "        if self.blacklist and not is_test:\n",
        "            print(f\"   [Dataset] {len(self.blacklist)}ê°œì˜ í•™ìŠµ ì´ë¯¸ì§€ë¥¼ ì œì™¸í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "        if is_test:\n",
        "            test_images_dir = data_dir\n",
        "            try:\n",
        "                for img_name in os.listdir(test_images_dir):\n",
        "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        # (í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ëŠ” ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì ìš© ì•ˆ í•¨)\n",
        "                        self.images.append(os.path.join(test_images_dir, img_name))\n",
        "            except Exception as e:\n",
        "                print(f\"í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
        "        else: #is_train\n",
        "            driver_subset = driver_df[driver_df['subject'].isin(driver_list)]\n",
        "\n",
        "            for _, row in driver_subset.iterrows():\n",
        "                class_name = row['classname']\n",
        "                img_name = row['img']\n",
        "\n",
        "                # [ìˆ˜ì •] ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— íŒŒì¼ëª…ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ\n",
        "                if img_name in self.blacklist:\n",
        "                    continue\n",
        "\n",
        "                img_path = os.path.join(data_dir, class_name, img_name)\n",
        "                self.images.append(img_path)\n",
        "                class_idx = int(class_name[1:])\n",
        "                self.labels.append(class_idx)\n",
        "\n",
        "        print(f\"{'í…ŒìŠ¤íŠ¸' if is_test else 'ìš´ì „ì ' + str(len(driver_list))+'ëª…'}, ë°ì´í„° {len(self.images)}ê°œ ì´ë¯¸ì§€ ë¡œë“œ\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜: {img_path}, {e}\")\n",
        "            return None, None\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.is_test:\n",
        "            return image, os.path.basename(img_path)\n",
        "        else:\n",
        "            label = self.labels[idx]\n",
        "            return image, label\n",
        "\n",
        "# --- 4. ë°ì´í„° ì „ì²˜ë¦¬ (Transforms) ---\n",
        "team_transform_train = transforms.Compose([\n",
        "  transforms.Resize((img_size, img_size)),\n",
        "  transforms.RandomRotation(degrees=15),\n",
        "  # [ì¶”ê°€] ì¢Œìš° ë°˜ì „ì€ c1/c3, c2/c4ë¥¼ í—·ê°ˆë¦¬ê²Œ í•  ìˆ˜ ìˆì–´ ì œì™¸ (ë˜ëŠ” ë¼ë²¨ ìŠ¤ìœ„ì¹­ í•„ìš”)\n",
        "  # transforms.RandomHorizontalFlip(),\n",
        "  transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "team_transform_eval = transforms.Compose([\n",
        "  transforms.Resize((img_size, img_size)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- 5. í•™ìŠµ/ê²€ì¦ í—¬í¼ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼) ---\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        if inputs is None or labels is None:\n",
        "            continue\n",
        "\n",
        "        # (collate_fnì—ì„œ ë¹ˆ ë°°ì¹˜ê°€ ì˜¬ ìˆ˜ ìˆìŒ)\n",
        "        if inputs.nelement() == 0:\n",
        "            continue\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct_predictions += torch.sum(preds == labels.data)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    if total_samples == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = (correct_predictions.double() / total_samples) * 100\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
        "            if inputs is None or labels is None:\n",
        "                continue\n",
        "\n",
        "            if inputs.nelement() == 0:\n",
        "                continue\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_predictions += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "    if total_samples == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = (correct_predictions.double() / total_samples) * 100\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "\n",
        "# --- 6. í•µì‹¬ í›ˆë ¨ í•¨ìˆ˜ (train_fold) [ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° ì¶”ê°€] ---\n",
        "def train_fold(fold_idx, train_drivers, val_drivers, blacklist_files): # [ìˆ˜ì •]\n",
        "    \"\"\"í•œ í´ë“œ í•™ìŠµ (2ë‹¨ê³„ ë¯¸ì„¸ ì¡°ì • ì ìš©)\"\"\"\n",
        "\n",
        "    print(f\"\\n========== Fold {fold_idx}/{n_folds} ==========\")\n",
        "\n",
        "    # --- ë°ì´í„°ì…‹ ë° ë¡œë” ì¤€ë¹„ ([ìˆ˜ì •] blacklist_files ì „ë‹¬) ---\n",
        "    train_dataset = DriverDataset(\n",
        "        train_dir, driver_df, train_drivers,\n",
        "        transform=team_transform_train, is_test=False,\n",
        "        blacklist_files=blacklist_files # [ìˆ˜ì •]\n",
        "    )\n",
        "    val_dataset = DriverDataset(\n",
        "        train_dir, driver_df, val_drivers,\n",
        "        transform=team_transform_eval, is_test=False,\n",
        "        blacklist_files=blacklist_files # [ìˆ˜ì •]\n",
        "    )\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "        if not batch: return torch.Tensor(), torch.Tensor()\n",
        "        return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, collate_fn=collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    print(f\"í•™ìŠµ ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")\n",
        "    print(f\"ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}\")\n",
        "\n",
        "    # --- ëª¨ë¸ ë¡œë“œ ---\n",
        "    model = timm.create_model(\n",
        "        'resnet50',\n",
        "        pretrained=True,\n",
        "        num_classes = num_classes\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ==========================================================\n",
        "    # 1ë‹¨ê³„: ë¶„ë¥˜ê¸°(Head)ë§Œ í•™ìŠµ (íŠ¹ì§• ì¶”ì¶œê¸°ëŠ” ë™ê²°)\n",
        "    # ==========================================================\n",
        "    print(\"\\n--- 1ë‹¨ê³„: ë¶„ë¥˜ê¸°(Head) í•™ìŠµ ì‹œì‘ ---\")\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    optimizer_head = optim.Adam(model.fc.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler_head = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer_head, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "    num_head_epochs = 10\n",
        "\n",
        "    for epoch in range(num_head_epochs):\n",
        "        print(f'\\n[Head Training] Epoch {epoch+1}/{num_head_epochs}')\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer_head, device)\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "        scheduler_head.step(val_loss)\n",
        "        print(f'  > Head Epoch {epoch+1}: Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # ==========================================================\n",
        "    # 2ë‹¨ê³„: ì „ì²´ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • (ì „ì²´ ë™ê²° í•´ì œ)\n",
        "    # ==========================================================\n",
        "    print(\"\\n--- 2ë‹¨ê³„: ì „ì²´ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • ì‹œì‘ ---\")\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    fine_tune_lr = learning_rate / 10  # (0.0001)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=fine_tune_lr, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=7\n",
        "    )\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_path = f'best_resnet50_fold{fold_idx}.pth'\n",
        "\n",
        "    remaining_epochs = num_epochs - num_head_epochs\n",
        "\n",
        "    for epoch in range(remaining_epochs):\n",
        "        current_epoch = epoch + 1 + num_head_epochs\n",
        "        print(f'\\n[Fine-Tuning] Epoch {current_epoch}/{num_epochs}')\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f'\\nğŸ“Š Epoch {current_epoch} ê²°ê³¼:')\n",
        "        print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
        "        print(f'  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'fold': fold_idx,\n",
        "                'epoch': current_epoch - 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc,\n",
        "            }, best_model_path)\n",
        "            print(f'  âœ“ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥! (Val Loss: {val_loss:.4f})')\n",
        "\n",
        "    print(f\"\\nâœ“ Fold {fold_idx} í•™ìŠµ ì™„ë£Œ! ìµœì € ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'fold': fold_idx,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'best_val_acc': max(history['val_acc']) if history['val_acc'] else 0.0,\n",
        "        'model_path': best_model_path\n",
        "    }\n",
        "\n",
        "# --- 7. ë©”ì¸ ì‹¤í–‰ [í•µì‹¬ ìˆ˜ì •: 5-Fold ì „ì²´ ì‹¤í–‰] ---\n",
        "\n",
        "# [ìˆ˜ì •] 1ë‹¨ê³„ ë¶„ì„ì—ì„œ ì°¾ì€ 'ë‚˜ìœ ë¼ë²¨' 50ê°œ ë¦¬ìŠ¤íŠ¸\n",
        "BLACKLIST_FILES = [\n",
        "    # Row 1 (True: c7, Pred: c0)\n",
        "    'img_88198.jpg', 'img_5883.jpg', 'img_21045.jpg', 'img_58599.jpg', 'img_65624.jpg',\n",
        "    # Row 2 (True: c2, Pred: c5)\n",
        "    'img_101662.jpg', 'img_57065.jpg', 'img_55392.jpg', 'img_101830.jpg', 'img_57106.jpg',\n",
        "    # Row 3 (True: c0, Pred: c2)\n",
        "    'img_77353.jpg', 'img_77264.jpg', 'img_77364.jpg', 'img_77387.jpg', 'img_77322.jpg',\n",
        "    # Row 4 (True: c0, Pred: c2)\n",
        "    'img_100361.jpg', 'img_100130.jpg', 'img_100122.jpg', 'img_100150.jpg', 'img_100170.jpg',\n",
        "    # Row 5 (True: c0, Pred: c3)\n",
        "    'img_22240.jpg', 'img_22288.jpg', 'img_22263.jpg', 'img_22285.jpg', 'img_22267.jpg',\n",
        "    # Row 6 (True: c0, Pred: c4) - ëª…ë°±í•œ ë¼ë²¨ ì˜¤ë¥˜\n",
        "    'img_8576.jpg', 'img_8494.jpg', 'img_8556.jpg', 'img_8524.jpg', 'img_8471.jpg',\n",
        "    # Row 7 (True: c0, Pred: c4) - ëª…ë°±í•œ ë¼ë²¨ ì˜¤ë¥˜\n",
        "    'img_41639.jpg', 'img_41620.jpg', 'img_41648.jpg', 'img_41656.jpg', 'img_41673.jpg',\n",
        "    # Row 8 (True: c0, Pred: c4) - ëª…ë°±í•œ ë¼ë²¨ ì˜¤ë¥˜\n",
        "    'img_87271.jpg', 'img_87211.jpg', 'img_87258.jpg', 'img_87192.jpg', 'img_87220.jpg',\n",
        "    # Row 9 (True: c0, Pred: c4) - ëª…ë°±í•œ ë¼ë²¨ ì˜¤ë¥˜\n",
        "    'img_69192.jpg', 'img_69187.jpg', 'img_69171.jpg', 'img_69176.jpg', 'img_69168.jpg',\n",
        "    # Row 10 (True: c0, Pred: c4) - ëª…ë°±í•œ ë¼ë²¨ ì˜¤ë¥˜\n",
        "    'img_8003.jpg', 'img_8022.jpg', 'img_8032.jpg', 'img_8055.jpg', 'img_8065.jpg'\n",
        "]\n",
        "print(f\"ë°ì´í„° í´ë Œì§•: {len(BLACKLIST_FILES)}ê°œì˜ ì˜ëª»ëœ ë¼ë²¨ ì´ë¯¸ì§€ë¥¼ í›ˆë ¨ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "all_fold_results = []\n",
        "try:\n",
        "    # [ìˆ˜ì •] fold_splits[0] ëŒ€ì‹  5ê°œ Fold ì „ì²´ë¥¼ ë£¨í”„ë¡œ ì‹¤í–‰\n",
        "    for fold_info in fold_splits:\n",
        "        fold_idx = fold_info['fold']\n",
        "        train_drivers = fold_info['train_drivers']\n",
        "        val_drivers = fold_info['val_drivers']\n",
        "\n",
        "        # [ìˆ˜ì •] train_foldì— blacklist ì „ë‹¬\n",
        "        fold_result = train_fold(fold_idx, train_drivers, val_drivers, BLACKLIST_FILES)\n",
        "        all_fold_results.append(fold_result)\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"âœ… 5-Fold ì „ì²´ í•™ìŠµ ì™„ë£Œ!\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    total_val_loss = 0\n",
        "    total_val_acc = 0\n",
        "    for result in all_fold_results:\n",
        "        print(f\"  Fold {result['fold']}: ìµœì € Val Loss: {result['best_val_loss']:.4f} | ìµœê³  Val Acc: {result['best_val_acc']:.2f}%\")\n",
        "        total_val_loss += result['best_val_loss']\n",
        "        total_val_acc += result['best_val_acc']\n",
        "\n",
        "    print(f\"\\n  > 5-Fold í‰ê·  ìµœì € Val Loss: {total_val_loss / n_folds:.4f}\")\n",
        "    print(f\"  > 5-Fold í‰ê·  ìµœê³  Val Acc: {total_val_acc / n_folds:.2f}%\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nğŸš¨ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "\n",
        "# --- 8. 5-Fold ì•™ìƒë¸” ì˜ˆì¸¡ ë° ì œì¶œ [í•µì‹¬ ìˆ˜ì •] ---\n",
        "if not all_fold_results:\n",
        "    print(\"ğŸš¨ ì˜¤ë¥˜: í•™ìŠµëœ í´ë“œ ê²°ê³¼ê°€ ì—†ì–´ ì˜ˆì¸¡ì„ ìƒëµí•©ë‹ˆë‹¤.\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"ğŸ”® 5-Fold ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # --- 8-1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ (í•œ ë²ˆë§Œ) ---\n",
        "    test_dataset = DriverDataset(\n",
        "        test_dir, driver_df, [],\n",
        "        transform=team_transform_eval, is_test=True\n",
        "    )\n",
        "\n",
        "    def collate_fn_test(batch):\n",
        "        batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "        if not batch: return torch.Tensor(), []\n",
        "        images, filenames = zip(*batch)\n",
        "        return torch.stack(images), list(filenames)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, collate_fn=collate_fn_test\n",
        "    )\n",
        "    print(f\"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_dataset)}ê°œ\")\n",
        "\n",
        "    # --- 8-2. 5ê°œ ëª¨ë¸ë¡œ ê°ê° ì˜ˆì¸¡ ìˆ˜í–‰ ---\n",
        "    all_folds_predictions = []\n",
        "\n",
        "    for result in all_fold_results:\n",
        "        fold_idx = result['fold']\n",
        "        model_path = result['model_path']\n",
        "\n",
        "        print(f\"\\n  Fold {fold_idx} ì˜ˆì¸¡ ì¤‘ (ëª¨ë¸: {model_path})...\")\n",
        "\n",
        "        # ëª¨ë¸ ë¼ˆëŒ€ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
        "        model = timm.create_model('resnet50', pretrained=False, num_classes=num_classes)\n",
        "        try:\n",
        "            checkpoint = torch.load(model_path, map_location=device)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        except Exception as e:\n",
        "            print(f\"ğŸš¨ Fold {fold_idx} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "            continue # ì´ í´ë“œëŠ” ê±´ë„ˆë›°ê¸°\n",
        "\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        fold_predictions = []\n",
        "        img_names_this_fold = [] # ì´ë¯¸ì§€ ì´ë¦„ì€ ì²« ë²ˆì§¸ í´ë“œì—ì„œë§Œ ê°€ì ¸ì˜´\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, filenames in tqdm(test_loader, desc=f'Fold {fold_idx} ì˜ˆì¸¡'):\n",
        "                if images.nelement() == 0: continue\n",
        "\n",
        "                images = images.to(device)\n",
        "                outputs = model(images)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "                fold_predictions.append(probs.cpu().numpy())\n",
        "\n",
        "                # [ìˆ˜ì •] ì´ë¯¸ì§€ ì´ë¦„ì€ ì²« ë²ˆì§¸ í´ë“œì—ì„œë§Œ ì €ì¥\n",
        "                if fold_idx == 1:\n",
        "                    img_names_this_fold.extend(filenames)\n",
        "\n",
        "        all_folds_predictions.append(np.vstack(fold_predictions))\n",
        "\n",
        "        # [ìˆ˜ì •] ì²« ë²ˆì§¸ í´ë“œì—ì„œë§Œ ì´ë¯¸ì§€ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ì •\n",
        "        if fold_idx == 1:\n",
        "            final_img_names = img_names_this_fold\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # --- 8-3. ì˜ˆì¸¡ ê²°ê³¼ ì•™ìƒë¸” (í‰ê· ) ---\n",
        "    if not all_folds_predictions:\n",
        "        print(\"ğŸš¨ ì˜¤ë¥˜: ìœ íš¨í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        print(f\"\\nâœ“ {len(all_folds_predictions)}ê°œ Fold ì˜ˆì¸¡ ì™„ë£Œ. ê²°ê³¼ ì•™ìƒë¸”(í‰ê· ) ì¤‘...\")\n",
        "\n",
        "        # (N_folds, N_images, N_classes) í˜•íƒœì˜ ë°°ì—´\n",
        "        all_folds_predictions_np = np.array(all_folds_predictions)\n",
        "\n",
        "        # [í•µì‹¬] 0ë²ˆ ì¶•(Fold ì¶•)ì„ ê¸°ì¤€ìœ¼ë¡œ í‰ê·  ê³„ì‚°\n",
        "        final_predictions_avg = np.mean(all_folds_predictions_np, axis=0)\n",
        "\n",
        "        print(f\"âœ“ ì•™ìƒë¸” ì™„ë£Œ: {final_predictions_avg.shape}\")\n",
        "\n",
        "        # --- 8-4. Submission íŒŒì¼ ìƒì„± ---\n",
        "        class_cols = [f'c{i}' for i in range(num_classes)]\n",
        "        submission_data = {'img': final_img_names}\n",
        "        for i, col in enumerate(class_cols):\n",
        "            submission_data[col] = final_predictions_avg[:, i]\n",
        "\n",
        "        submission = pd.DataFrame(submission_data)\n",
        "\n",
        "        # [ìˆ˜ì •] íŒŒì¼ ì´ë¦„ ë³€ê²½\n",
        "        submission_file = 'resnet50_5fold_ensemble_submission.csv'\n",
        "        submission.to_csv(submission_file, index=False)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"âœ… 5-Fold ì•™ìƒë¸” Submission íŒŒì¼ ìƒì„± ì™„ë£Œ: {submission_file}\")\n",
        "        print(f\"âœ… ì´ {len(submission)}ê°œ ì´ë¯¸ì§€ ì˜ˆì¸¡\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"\\nğŸ“‹ Submission ìƒ˜í”Œ:\")\n",
        "        print(submission.head())\n",
        "\n",
        "# --- 9. ì˜¤ë¶„ë¥˜ ë¶„ì„ [ì œê±°] ---\n",
        "# (ì˜¤ë¶„ë¥˜ ë¶„ì„(1ë‹¨ê³„)ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆê³ ,\n",
        "#  ë¸”ë™ë¦¬ìŠ¤íŠ¸(2ë‹¨ê³„)ëŠ” 7ë²ˆ ì„¹ì…˜ì— ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.)\n",
        "# (ìƒˆë¡œ í›ˆë ¨ëœ ëª¨ë¸ë¡œ ë‹¤ì‹œ ë¶„ì„í•˜ê³  ì‹¶ë‹¤ë©´ ì´ì „ 9ë²ˆ ì½”ë“œë¥¼ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.)"
      ],
      "metadata": {
        "id": "_qANq3-LZiRK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "c7f206f9-77f2-47e1-c03e-1f4a784f44e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n",
            "GPU: Tesla T4\n",
            "Train directory: /kaggle/input/state-farm-distracted-driver-detection/imgs/train\n",
            "Test directory: /kaggle/input/state-farm-distracted-driver-detection/imgs/test\n",
            "ì´ë¯¸ì§€ í¬ê¸°: 224x224\n",
            "ë°°ì¹˜ í¬ê¸°: 32\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/state-farm-distracted-driver-detection/driver_imgs_list.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4165494183.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ë°°ì¹˜ í¬ê¸°: {batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdriver_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_csv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ê³ ìœ  ìš´ì „ì ìˆ˜: {driver_df['subject'].nunique()}ëª…\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/state-farm-distracted-driver-detection/driver_imgs_list.csv'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1B1m2zMimQMDsM7_sPdFcbKhfVlXkuDa4",
      "authorship_tag": "ABX9TyPCTJ4ZV1y88Gzlbls3H7dd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}