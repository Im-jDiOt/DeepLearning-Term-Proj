{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DJe4OjEdap15iVjgXelZj7JLplfKQKpu",
      "authorship_tag": "ABX9TyNIO9ZrUAAN8bZx1zXbOk2g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Im-jDiOt/DeepLearning-Term-Proj/blob/feature-resnet/5%EC%B0%A8%EC%8B%A4%ED%96%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import timm  # timm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "\n",
        "# --- 1. ë°ì´í„° ë¡œë“œ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ---\n",
        "base_dir = r'/content/drive/MyDrive/Colab Notebooks'\n",
        "# [ìˆ˜ì •] 'data' í´ë” ì œê±°\n",
        "driver_csv_path = os.path.join(base_dir, 'driver_imgs_list.csv')\n",
        "# [ìˆ˜ì •] 'data/imgs' í´ë” ì œê±°\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# [ìˆ˜ì •] ResNet-50ì˜ í‘œì¤€ ì…ë ¥ í¬ê¸°ëŠ” 224x224 ì…ë‹ˆë‹¤.\n",
        "img_size = 224\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "num_epochs = 50\n",
        "learning_rate = 0.001\n",
        "num_workers = 0\n",
        "\n",
        "print(f\"Train directory: {train_dir}\")\n",
        "print(f\"Test directory: {test_dir}\")\n",
        "print(f\"ì´ë¯¸ì§€ í¬ê¸°: {img_size}x{img_size}\")\n",
        "print(f\"ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
        "\n",
        "driver_df = pd.read_csv(driver_csv_path)\n",
        "print(f\"ê³ ìœ  ìš´ì „ì ìˆ˜: {driver_df['subject'].nunique()}ëª…\")\n",
        "\n",
        "# --- 2. 5-Fold êµì°¨ ê²€ì¦ (ìš´ì „ì ê¸°ì¤€ ë¶„í• ) ---\n",
        "all_drivers = sorted(driver_df['subject'].unique())\n",
        "n_folds = 5\n",
        "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "fold_splits = []\n",
        "print(\"K-Fold ìš´ì „ì ë¶„í• :\")\n",
        "for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(all_drivers)):\n",
        "    train_drivers = [all_drivers[i] for i in train_indices]\n",
        "    val_drivers = [all_drivers[i] for i in val_indices]\n",
        "\n",
        "    fold_splits.append({\n",
        "        'fold': fold_idx+1,\n",
        "        'train_drivers': train_drivers,\n",
        "        'val_drivers': val_drivers\n",
        "    })\n",
        "\n",
        "    # (ë¡œê·¸ ì¶œë ¥ì€ ê°„ê²°í•˜ê²Œ)\n",
        "    train_imgs = driver_df[driver_df['subject'].isin(train_drivers)]\n",
        "    val_imgs = driver_df[driver_df['subject'].isin(val_drivers)]\n",
        "    print(f\"  Fold {fold_idx+1}: í•™ìŠµ {len(train_imgs)}ê°œ | ê²€ì¦ {len(val_imgs)}ê°œ\")\n",
        "\n",
        "\n",
        "# --- 3. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ (DriverDataset) ---\n",
        "class DriverDataset(Dataset):\n",
        "    \"\"\"ìš´ì „ì í–‰ë™ ë°ì´í„°ì…‹\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, driver_df, driver_list, transform=None, is_test=False):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        if is_test:\n",
        "            test_images_dir = data_dir\n",
        "            # (OS Error ë°©ì§€ë¥¼ ìœ„í•´ os.listdir í›„ .endswith ì²´í¬)\n",
        "            try:\n",
        "                for img_name in os.listdir(test_images_dir):\n",
        "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        self.images.append(os.path.join(test_images_dir, img_name))\n",
        "            except Exception as e:\n",
        "                print(f\"í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
        "        else: #is_train\n",
        "            driver_subset = driver_df[driver_df['subject'].isin(driver_list)]\n",
        "\n",
        "            for _, row in driver_subset.iterrows():\n",
        "                class_name = row['classname']\n",
        "                img_name = row['img']\n",
        "                img_path = os.path.join(data_dir, class_name, img_name)\n",
        "                self.images.append(img_path)\n",
        "                class_idx = int(class_name[1:])\n",
        "                self.labels.append(class_idx)\n",
        "\n",
        "        print(f\"{'í…ŒìŠ¤íŠ¸' if is_test else 'ìš´ì „ì ' + str(len(driver_list))+'ëª…'}, ë°ì´í„° {len(self.images)}ê°œ ì´ë¯¸ì§€ ë¡œë“œ\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜: {img_path}, {e}\")\n",
        "            return None, None # (ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.is_test:\n",
        "            return image, os.path.basename(img_path)\n",
        "        else:\n",
        "            label = self.labels[idx]\n",
        "            return image, label\n",
        "\n",
        "# --- 4. ë°ì´í„° ì „ì²˜ë¦¬ (Transforms) ---\n",
        "# (íŒ€ì›ë¶„ì˜ 'team_transform' íŒŒì´í”„ë¼ì¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n",
        "team_transform_train = transforms.Compose([\n",
        "\ttransforms.Resize((img_size, img_size)),\n",
        "\ttransforms.RandomRotation(degrees=15),\n",
        "\ttransforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05),\n",
        "\ttransforms.ToTensor(),\n",
        "\ttransforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "team_transform_eval = transforms.Compose([\n",
        "\ttransforms.Resize((img_size, img_size)),\n",
        "\ttransforms.ToTensor(),\n",
        "\ttransforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- 5. í•™ìŠµ/ê²€ì¦ í—¬í¼ í•¨ìˆ˜ ---\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\" í•œ ì—í­(Epoch) ë™ì•ˆ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        # (ë°ì´í„°ì…‹ ì˜¤ë¥˜ ì‹œ ë°°ì¹˜ ìŠ¤í‚µ)\n",
        "        if inputs is None or labels is None:\n",
        "            continue\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct_predictions += torch.sum(preds == labels.data)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    if total_samples == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = (correct_predictions.double() / total_samples) * 100\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\" ê²€ì¦ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
        "            if inputs is None or labels is None:\n",
        "                continue\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_predictions += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "    if total_samples == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = (correct_predictions.double() / total_samples) * 100\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "\n",
        "# --- 6. í•µì‹¬ í›ˆë ¨ í•¨ìˆ˜ (train_fold) [2ë‹¨ê³„ ë¯¸ì„¸ ì¡°ì • ì ìš©] ---\n",
        "def train_fold(fold_idx, train_drivers, val_drivers):\n",
        "    \"\"\"í•œ í´ë“œ í•™ìŠµ (2ë‹¨ê³„ ë¯¸ì„¸ ì¡°ì • ì ìš©)\"\"\"\n",
        "\n",
        "    print(f\"\\n========== Fold {fold_idx}/{n_folds} ==========\")\n",
        "\n",
        "    # --- ë°ì´í„°ì…‹ ë° ë¡œë” ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼) ---\n",
        "    train_dataset = DriverDataset(\n",
        "        train_dir, driver_df, train_drivers,\n",
        "        transform=team_transform_train, is_test=False\n",
        "    )\n",
        "    val_dataset = DriverDataset(\n",
        "        train_dir, driver_df, val_drivers,\n",
        "        transform=team_transform_eval, is_test=False\n",
        "    )\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "        if not batch: return torch.Tensor(), torch.Tensor()\n",
        "        return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, collate_fn=collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    print(f\"í•™ìŠµ ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")\n",
        "    print(f\"ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}\")\n",
        "\n",
        "    # --- ëª¨ë¸ ë¡œë“œ ---\n",
        "    model = timm.create_model(\n",
        "        'resnet50',\n",
        "        pretrained=True,\n",
        "        num_classes = num_classes\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ==========================================================\n",
        "    # 1ë‹¨ê³„: ë¶„ë¥˜ê¸°(Head)ë§Œ í•™ìŠµ (íŠ¹ì§• ì¶”ì¶œê¸°ëŠ” ë™ê²°)\n",
        "    # ==========================================================\n",
        "    print(\"\\n--- 1ë‹¨ê³„: ë¶„ë¥˜ê¸°(Head) í•™ìŠµ ì‹œì‘ ---\")\n",
        "\n",
        "    # 1. ëª¨ë“  ë ˆì´ì–´ ë™ê²°\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 2. ResNet-50ì˜ ë§ˆì§€ë§‰ 'fc' ë ˆì´ì–´ë§Œ ë™ê²° í•´ì œ\n",
        "    #    (timmì˜ resnet50ì€ ë§ˆì§€ë§‰ ì¸µ ì´ë¦„ì´ 'fc' ì…ë‹ˆë‹¤)\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # 3. Head ì˜µí‹°ë§ˆì´ì € (í•™ìŠµí•  íŒŒë¼ë¯¸í„°ë§Œ ì „ë‹¬)\n",
        "    optimizer_head = optim.Adam(model.fc.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler_head = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer_head, mode='min', factor=0.5, patience=5 # HeadëŠ” ì¡°ê¸ˆ ë” ì§§ê²Œ\n",
        "    )\n",
        "\n",
        "    # (Head í•™ìŠµì€ ì „ì²´ ì—í­ì˜ ì¼ë¶€ë§Œ í• ë‹¹, ì˜ˆ: 20% ë˜ëŠ” 10 ì—í­)\n",
        "    num_head_epochs = 10\n",
        "\n",
        "    for epoch in range(num_head_epochs):\n",
        "        print(f'\\n[Head Training] Epoch {epoch+1}/{num_head_epochs}')\n",
        "        print('-' * 70)\n",
        "\n",
        "        # (ê¸°ì¡´ í•™ìŠµ/ê²€ì¦ í•¨ìˆ˜ ì¬ì‚¬ìš©)\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer_head, device)\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        scheduler_head.step(val_loss)\n",
        "\n",
        "        print(f'\\nğŸ“Š Head Epoch {epoch+1} ê²°ê³¼:')\n",
        "        print(f'    Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
        "        print(f'   Val Loss: Â  {val_loss:.4f} | Val Acc: Â  {val_acc:.2f}%')\n",
        "\n",
        "\n",
        "    # ==========================================================\n",
        "    # 2ë‹¨ê³„: ì „ì²´ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • (ì „ì²´ ë™ê²° í•´ì œ)\n",
        "    # ==========================================================\n",
        "    print(\"\\n--- 2ë‹¨ê³„: ì „ì²´ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • ì‹œì‘ ---\")\n",
        "\n",
        "    # 1. ëª¨ë¸ ì „ì²´ ë™ê²° í•´ì œ\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # 2. ì „ì²´ ëª¨ë¸ ì˜µí‹°ë§ˆì´ì € (ë§¤ìš° ë‚®ì€ í•™ìŠµë¥  ì„¤ì •)\n",
        "    fine_tune_lr = learning_rate / 10  # (ì˜ˆ: 0.001 -> 0.0001)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=fine_tune_lr, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=7 # (ê¸°ì¡´ ì„¤ì •)\n",
        "    )\n",
        "\n",
        "    # (History ë° Best Loss ì´ˆê¸°í™” - 2ë‹¨ê³„ë¶€í„° ì§„ì§œ ì‹œì‘)\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_path = f'best_resnet50_fold{fold_idx}.pth'\n",
        "\n",
        "    # (ë‚¨ì€ ì—í­ ë™ì•ˆ 2ë‹¨ê³„ í•™ìŠµ ì§„í–‰)\n",
        "    remaining_epochs = num_epochs - num_head_epochs\n",
        "\n",
        "    for epoch in range(remaining_epochs):\n",
        "        # (ì—í­ ì¹´ìš´íŠ¸ëŠ” ì´ì–´ì„œ í‘œì‹œ)\n",
        "        current_epoch = epoch + 1 + num_head_epochs\n",
        "        print(f'\\n[Fine-Tuning] Epoch {current_epoch}/{num_epochs}')\n",
        "        print('-' * 70)\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f'\\nğŸ“Š Epoch {current_epoch} ê²°ê³¼:')\n",
        "        print(f' Â Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
        "        print(f' Â Val Loss: Â  {val_loss:.4f} | Val Acc: Â  {val_acc:.2f}%')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # (Best ëª¨ë¸ ì €ì¥ì€ ì—¬ê¸°ì„œë§Œ ìˆ˜í–‰)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'fold': fold_idx,\n",
        "                'epoch': current_epoch - 1, # (0-based index)\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc,\n",
        "                'history': history\n",
        "            }, best_model_path)\n",
        "            print(f' Â âœ“ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥! (Val Loss: {val_loss:.4f})')\n",
        "\n",
        "    print(f\"\\nâœ“ Fold {fold_idx} í•™ìŠµ ì™„ë£Œ! ìµœì € ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'fold': fold_idx,\n",
        "        'history': history,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        # (best_val_accëŠ” history['val_acc']ê°€ 2ë‹¨ê³„ë§Œ í¬í•¨í•˜ë¯€ë¡œ ìˆ˜ì •)\n",
        "        'best_val_acc': max(history['val_acc']) if history['val_acc'] else 0.0,\n",
        "        'model_path': best_model_path\n",
        "    }\n",
        "# --- 7. ë©”ì¸ ì‹¤í–‰ (Fold 2ë§Œ ì‹¤í–‰) ---\n",
        "all_fold_results = []\n",
        "try:\n",
        "    # (íŒ€ì›ë¶„ì˜ ì „ëµëŒ€ë¡œ Fold 2 (ì¸ë±ìŠ¤ 1)ë§Œ ìš°ì„  ì‹¤í–‰)\n",
        "    fold_info = fold_splits[0]\n",
        "    fold_idx = fold_info['fold']\n",
        "    train_drivers = fold_info['train_drivers']\n",
        "    val_drivers = fold_info['val_drivers']\n",
        "\n",
        "    fold_result = train_fold(fold_idx, train_drivers, val_drivers)\n",
        "    all_fold_results.append(fold_result)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n--- ìµœì¢… ê²°ê³¼ (Fold {fold_result['fold']}) ---\")\n",
        "    print(f\"  ìµœì € Val Loss: {fold_result['best_val_loss']:.4f}\")\n",
        "    print(f\"  ìµœê³  Val Acc: {fold_result['best_val_acc']:.2f}%\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nğŸš¨ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\n",
        "\n",
        "# --- 8. ë‹¨ì¼ ëª¨ë¸ ì˜ˆì¸¡ ë° ì œì¶œ ---\n",
        "if not all_fold_results:\n",
        "    print(\"ğŸš¨ ì˜¤ë¥˜: í•™ìŠµëœ í´ë“œ ê²°ê³¼ê°€ ì—†ì–´ ì˜ˆì¸¡ì„ ìƒëµí•©ë‹ˆë‹¤.\")\n",
        "else:\n",
        "    result = all_fold_results[0] # (Fold 1ì˜ ê²°ê³¼)\n",
        "    fold_idx = result['fold']\n",
        "    model_path = result['model_path']\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"ğŸ”® Fold {fold_idx} ë‹¨ì¼ ëª¨ë¸ ì˜ˆì¸¡ ì‹œì‘\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # --- 8-1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ---\n",
        "    test_dataset = DriverDataset(\n",
        "        test_dir, driver_df, [],\n",
        "        transform=team_transform_eval, is_test=True\n",
        "    )\n",
        "\n",
        "    def collate_fn_test(batch):\n",
        "        batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "        if not batch: return torch.Tensor(), []\n",
        "        images, filenames = zip(*batch)\n",
        "        return torch.stack(images), list(filenames)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, collate_fn=collate_fn_test\n",
        "    )\n",
        "    print(f\"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_dataset)}ê°œ\")\n",
        "\n",
        "    # --- 8-2. ëª¨ë¸ ë¡œë“œ ---\n",
        "    model = timm.create_model('resnet50', pretrained=False, num_classes=num_classes)\n",
        "    print(f\"\\nğŸ“ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}...\")\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"âœ“ Fold {fold_idx} ëª¨ë¸ (Epoch {checkpoint['epoch']+1}, Val Loss: {checkpoint.get('val_loss', 'N/A'):.4f}) ë¡œë“œ ì™„ë£Œ.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "        raise\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # --- 8-3. ì˜ˆì¸¡ ìˆ˜í–‰ ---\n",
        "    predictions = []\n",
        "    img_names = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, filenames in tqdm(test_loader, desc=f'Fold {fold_idx} ì˜ˆì¸¡'):\n",
        "            if images.nelement() == 0: continue\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            predictions.append(probs.cpu().numpy())\n",
        "            img_names.extend(filenames)\n",
        "\n",
        "    final_predictions = np.vstack(predictions)\n",
        "    print(f\"\\nâœ“ ì˜ˆì¸¡ ì™„ë£Œ: {final_predictions.shape}\")\n",
        "\n",
        "    # --- 8-4. Submission íŒŒì¼ ìƒì„± ---\n",
        "    class_cols = [f'c{i}' for i in range(num_classes)]\n",
        "    submission_data = {'img': img_names}\n",
        "    for i, col in enumerate(class_cols):\n",
        "        submission_data[col] = final_predictions[:, i]\n",
        "\n",
        "    submission = pd.DataFrame(submission_data)\n",
        "    submission_file = f'resnet50_fold{fold_idx}_single_model_submission.csv'\n",
        "    submission.to_csv(submission_file, index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"âœ… Submission íŒŒì¼ ìƒì„± ì™„ë£Œ: {submission_file}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(submission.head())\n",
        "\n",
        "    # --- 9. ë°ì´í„° í´ë Œì§•: ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ì‹œê°í™” ---\n",
        "\n",
        "# (ì‹œê°í™”ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ ì´ë¦„, í•¨ìˆ˜ ë“± ì •ì˜)\n",
        "CLASS_NAMES_MAP = {\n",
        "    0: 'c0: safe driving',\n",
        "    1: 'c1: texting - right',\n",
        "    2: 'c2: talking on phone - right',\n",
        "    3: 'c3: texting - left',\n",
        "    4: 'c4: talking on phone - left',\n",
        "    5: 'c5: operating the radio',\n",
        "    6: 'c6: drinking',\n",
        "    7: 'c7: reaching behind',\n",
        "    8: 'c8: hair and makeup',\n",
        "    9: 'c9: talking to passenger'\n",
        "}\n",
        "\n",
        "def denormalize(tensor_image):\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = tensor_image.permute(1, 2, 0).cpu().numpy()\n",
        "    img = (img * std) + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return img\n",
        "\n",
        "class CleansingDataset(DriverDataset):\n",
        "    def __init__(self, data_dir, driver_df, driver_list, transform=None):\n",
        "        super().__init__(data_dir, driver_df, driver_list, transform, is_test=False)\n",
        "        print(f\"   [ë¶„ì„ìš©] CleansingDataset ë¡œë“œ: {len(self.images)}ê°œ\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            return None, None, None\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label, img_path\n",
        "\n",
        "def collate_fn_cleanse(batch):\n",
        "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "    if not batch:\n",
        "        return torch.Tensor(), torch.Tensor(), []\n",
        "    images, labels, paths = zip(*batch)\n",
        "    return torch.stack(images), torch.tensor(labels), list(paths)\n",
        "\n",
        "def find_incorrect_samples(model, data_loader, device):\n",
        "    model.eval()\n",
        "    incorrect_samples = []\n",
        "    correct_samples = []\n",
        "    print(f\"\\në°ì´í„° ë¡œë”ì—ì„œ ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ íƒìƒ‰ ì¤‘...\")\n",
        "    for images, labels, paths in tqdm(data_loader, desc=\"Finding Errors\"):\n",
        "        if images.nelement() == 0: continue\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "        for j in range(len(images)):\n",
        "            true_label = labels[j].item()\n",
        "            pred_label = preds[j].item()\n",
        "            sample_info = {\n",
        "                'path': paths[j],\n",
        "                'filename': os.path.basename(paths[j]),\n",
        "                'image_tensor': images[j],\n",
        "                'true_label': true_label,\n",
        "                'pred_label': pred_label\n",
        "            }\n",
        "            if pred_label == true_label:\n",
        "                correct_samples.append(sample_info)\n",
        "            else:\n",
        "                incorrect_samples.append(sample_info)\n",
        "    print(f\"  > ì´ {len(data_loader.dataset)}ê°œ ì¤‘, ì •ë‹µ: {len(correct_samples)}ê°œ | ì˜¤ë‹µ: {len(incorrect_samples)}ê°œ\")\n",
        "    return incorrect_samples, correct_samples\n",
        "\n",
        "def visualize_samples(samples, title, num_to_show=25):\n",
        "    if not samples:\n",
        "        print(f\"ì‹œê°í™”í•  ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤: {title}\")\n",
        "        return\n",
        "    print(f\"\\n--- {title} (ìµœëŒ€ {num_to_show}ê°œ) ---\")\n",
        "    rows = int(np.ceil(num_to_show / 5))\n",
        "    plt.figure(figsize=(20, 4 * rows))\n",
        "    for i, sample in enumerate(samples[:num_to_show]):\n",
        "        ax = plt.subplot(rows, 5, i + 1)\n",
        "        img = denormalize(sample['image_tensor'])\n",
        "        plt.imshow(img)\n",
        "        true_name = CLASS_NAMES_MAP.get(sample['true_label'], f\"c{sample['true_label']}\")\n",
        "        pred_name = CLASS_NAMES_MAP.get(sample['pred_label'], f\"c{sample['pred_label']}\")\n",
        "        plot_title = (\n",
        "            f\"File: {sample['filename']}\\n\"\n",
        "            f\"True: {true_name}\\n\"\n",
        "            f\"Pred: {pred_name}\"\n",
        "        )\n",
        "        ax.set_title(plot_title, fontsize=10, color='red' if 'Incorrect' in title else 'green')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- [ë©”ì¸ ì‹¤í–‰] ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ§ 9. ì˜¤ë¶„ë¥˜ ë°ì´í„° ë¶„ì„ ì‹œì‘ (Data Cleansing)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    # 1. Fold 1ì˜ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ\n",
        "    model_to_analyze_path = 'best_resnet50_fold1.pth'\n",
        "    fold_to_analyze_idx = 0 # (fold_splits[0] = Fold 1)\n",
        "\n",
        "    print(f\"ëª¨ë¸ ë¡œë“œ ì¤‘: {model_to_analyze_path}\")\n",
        "    # (ëª¨ë¸ ë¼ˆëŒ€ ìƒì„±)\n",
        "    model_for_analysis = timm.create_model('resnet50', pretrained=False, num_classes=num_classes)\n",
        "    checkpoint = torch.load(model_to_analyze_path, map_location=device)\n",
        "    model_for_analysis.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model_for_analysis = model_for_analysis.to(device)\n",
        "    model_for_analysis.eval()\n",
        "    print(\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\")\n",
        "\n",
        "    # 2. Fold 1ì˜ í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¡œë” (ì¦ê°•X, ì…”í”ŒX)\n",
        "    fold_info = fold_splits[fold_to_analyze_idx]\n",
        "\n",
        "    clean_check_train_dataset = CleansingDataset(\n",
        "        train_dir, driver_df, fold_info['train_drivers'],\n",
        "        transform=team_transform_eval\n",
        "    )\n",
        "    clean_check_train_loader = DataLoader(\n",
        "        clean_check_train_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, collate_fn=collate_fn_cleanse\n",
        "    )\n",
        "\n",
        "    clean_check_val_dataset = CleansingDataset(\n",
        "        train_dir, driver_df, fold_info['val_drivers'],\n",
        "        transform=team_transform_eval\n",
        "    )\n",
        "    clean_check_val_loader = DataLoader(\n",
        "        clean_check_val_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, collate_fn=collate_fn_cleanse\n",
        "    )\n",
        "\n",
        "    # 3. ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ì°¾ê¸°\n",
        "    train_errors, _ = find_incorrect_samples(model_for_analysis, clean_check_train_loader, device)\n",
        "    val_errors, _ = find_incorrect_samples(model_for_analysis, clean_check_val_loader, device)\n",
        "\n",
        "    all_errors = train_errors + val_errors\n",
        "    print(f\"\\n>>> ì´ ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ìˆ˜ (Train + Val): {len(all_errors)}ê°œ\")\n",
        "\n",
        "    # 4. ì‹œê°í™”\n",
        "    visualize_samples(all_errors, f\"Fold {fold_to_analyze_idx+1} - Incorrect Samples (Train+Val)\", num_to_show=50)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ğŸš¨ ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼ '{model_to_analyze_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"   (ë¨¼ì € #1~#7 í›ˆë ¨ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ì €ì¥í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.)\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸš¨ ì˜¤ë¶„ë¥˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "AWrYewl1m-Ir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}