{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1O6plD7MLeNQBRNsBxY7UX_pZLbn7XCHt",
      "authorship_tag": "ABX9TyPa2Y0VaFUBLriczZoZpxmi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Im-jDiOt/DeepLearning-Term-Proj/blob/feature-resnet/1%EC%B0%A8%EC%8B%A4%ED%96%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import timm  # timm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "\n",
        "# --- 1. ë°ì´í„° ë¡œë“œ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ---\n",
        "base_dir = r'/content/drive/MyDrive/Colab Notebooks'\n",
        "# [ìˆ˜ì •] 'data' í´ë” ì œê±°\n",
        "driver_csv_path = os.path.join(base_dir, 'driver_imgs_list.csv')\n",
        "# [ìˆ˜ì •] 'data/imgs' í´ë” ì œê±°\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# [ìˆ˜ì •] ResNet-50ì˜ í‘œì¤€ ì…ë ¥ í¬ê¸°ëŠ” 224x224 ì…ë‹ˆë‹¤.\n",
        "img_size = 224\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "num_epochs = 50\n",
        "learning_rate = 0.001\n",
        "num_workers = 0\n",
        "\n",
        "print(f\"Train directory: {train_dir}\")\n",
        "print(f\"Test directory: {test_dir}\")\n",
        "print(f\"ì´ë¯¸ì§€ í¬ê¸°: {img_size}x{img_size}\")\n",
        "print(f\"ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
        "\n",
        "driver_df = pd.read_csv(driver_csv_path)\n",
        "print(f\"ê³ ìœ  ìš´ì „ì ìˆ˜: {driver_df['subject'].nunique()}ëª…\")\n",
        "\n",
        "# --- 2. 5-Fold êµì°¨ ê²€ì¦ (ìš´ì „ì ê¸°ì¤€ ë¶„í• ) ---\n",
        "all_drivers = sorted(driver_df['subject'].unique())\n",
        "n_folds = 5\n",
        "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "fold_splits = []\n",
        "print(\"K-Fold ìš´ì „ì ë¶„í• :\")\n",
        "for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(all_drivers)):\n",
        "    train_drivers = [all_drivers[i] for i in train_indices]\n",
        "    val_drivers = [all_drivers[i] for i in val_indices]\n",
        "\n",
        "    fold_splits.append({\n",
        "        'fold': fold_idx+1,\n",
        "        'train_drivers': train_drivers,\n",
        "        'val_drivers': val_drivers\n",
        "    })\n",
        "\n",
        "    # (ë¡œê·¸ ì¶œë ¥ì€ ê°„ê²°í•˜ê²Œ)\n",
        "    train_imgs = driver_df[driver_df['subject'].isin(train_drivers)]\n",
        "    val_imgs = driver_df[driver_df['subject'].isin(val_drivers)]\n",
        "    print(f\"  Fold {fold_idx+1}: í•™ìŠµ {len(train_imgs)}ê°œ | ê²€ì¦ {len(val_imgs)}ê°œ\")\n",
        "\n",
        "\n",
        "# --- 3. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ (DriverDataset) ---\n",
        "class DriverDataset(Dataset):\n",
        "    \"\"\"ìš´ì „ì í–‰ë™ ë°ì´í„°ì…‹\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, driver_df, driver_list, transform=None, is_test=False):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        if is_test:\n",
        "            test_images_dir = data_dir\n",
        "            # (OS Error ë°©ì§€ë¥¼ ìœ„í•´ os.listdir í›„ .endswith ì²´í¬)\n",
        "            try:\n",
        "                for img_name in os.listdir(test_images_dir):\n",
        "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        self.images.append(os.path.join(test_images_dir, img_name))\n",
        "            except Exception as e:\n",
        "                print(f\"í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
        "        else: #is_train\n",
        "            driver_subset = driver_df[driver_df['subject'].isin(driver_list)]\n",
        "\n",
        "            for _, row in driver_subset.iterrows():\n",
        "                class_name = row['classname']\n",
        "                img_name = row['img']\n",
        "                img_path = os.path.join(data_dir, class_name, img_name)\n",
        "                self.images.append(img_path)\n",
        "                class_idx = int(class_name[1:])\n",
        "                self.labels.append(class_idx)\n",
        "\n",
        "        print(f\"{'í…ŒìŠ¤íŠ¸' if is_test else 'ìš´ì „ì ' + str(len(driver_list))+'ëª…'}, ë°ì´í„° {len(self.images)}ê°œ ì´ë¯¸ì§€ ë¡œë“œ\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜: {img_path}, {e}\")\n",
        "            return None, None # (ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.is_test:\n",
        "            return image, os.path.basename(img_path)\n",
        "        else:\n",
        "            label = self.labels[idx]\n",
        "            return image, label\n",
        "\n",
        "# --- 4. ë°ì´í„° ì „ì²˜ë¦¬ (Transforms) ---\n",
        "# (íŒ€ì›ë¶„ì˜ 'team_transform' íŒŒì´í”„ë¼ì¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n",
        "team_transform_train = transforms.Compose([\n",
        "\ttransforms.Resize((img_size, img_size)),\n",
        "\ttransforms.RandomRotation(degrees=15),\n",
        "\ttransforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05),\n",
        "\ttransforms.ToTensor(),\n",
        "\ttransforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "team_transform_eval = transforms.Compose([\n",
        "\ttransforms.Resize((img_size, img_size)),\n",
        "\ttransforms.ToTensor(),\n",
        "\ttransforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- 5. í•™ìŠµ/ê²€ì¦ í—¬í¼ í•¨ìˆ˜ ---\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\" í•œ ì—í­(Epoch) ë™ì•ˆ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        # (ë°ì´í„°ì…‹ ì˜¤ë¥˜ ì‹œ ë°°ì¹˜ ìŠ¤í‚µ)\n",
        "        if inputs is None or labels is None:\n",
        "            continue\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct_predictions += torch.sum(preds == labels.data)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    if total_samples == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = (correct_predictions.double() / total_samples) * 100\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\" ê²€ì¦ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
        "            if inputs is None or labels is None:\n",
        "                continue\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_predictions += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "    if total_samples == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = (correct_predictions.double() / total_samples) * 100\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "\n",
        "# --- 6. í•µì‹¬ í›ˆë ¨ í•¨ìˆ˜ (train_fold) ---\n",
        "def train_fold(fold_idx, train_drivers, val_drivers):\n",
        "    \"\"\"í•œ í´ë“œ í•™ìŠµ (Log Loss ê¸°ì¤€)\"\"\"\n",
        "\n",
        "    print(f\"\\n========== Fold {fold_idx}/{n_folds} ==========\")\n",
        "\n",
        "    train_dataset = DriverDataset(\n",
        "        train_dir, driver_df, train_drivers,\n",
        "        transform=team_transform_train, is_test=False\n",
        "    )\n",
        "    val_dataset = DriverDataset(\n",
        "        train_dir, driver_df, val_drivers,\n",
        "        transform=team_transform_eval, is_test=False\n",
        "    )\n",
        "\n",
        "    # (DataLoaderì—ì„œ ì˜¤ë¥˜ ë°°ì¹˜ë¥¼ ìŠ¤í‚µí•˜ê¸° ìœ„í•œ collate_fn)\n",
        "    def collate_fn(batch):\n",
        "        batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "        if not batch: return torch.Tensor(), torch.Tensor()\n",
        "        return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, collate_fn=collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    print(f\"í•™ìŠµ ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")\n",
        "    print(f\"ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}\")\n",
        "\n",
        "    # --- [ëª¨ë¸ ìˆ˜ì •] Inception-v4 -> ResNet-50 ---\n",
        "    model = timm.create_model(\n",
        "        'resnet50',     # timmì—ì„œ ResNet-50 ëª¨ë¸ ì‚¬ìš©\n",
        "        pretrained=True,\n",
        "        num_classes = num_classes\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    # (íŒ€ì›ë¶„ì˜ í›ˆë ¨ ë°©ì‹(Adam + ReduceLROnPlateau) ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=7\n",
        "    )\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    # [ìˆ˜ì •] ëª¨ë¸ ì´ë¦„ ë³€ê²½\n",
        "    best_model_path = f'best_resnet50_fold{fold_idx}.pth'\n",
        "\n",
        "    # ì—í­ í•™ìŠµ\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 70)\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f'\\nğŸ“Š Epoch {epoch+1} ê²°ê³¼:')\n",
        "        print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
        "        print(f'  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'fold': fold_idx,\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc,\n",
        "                'history': history\n",
        "            }, best_model_path)\n",
        "            print(f'  âœ“ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥! (Val Loss: {val_loss:.4f})')\n",
        "\n",
        "    print(f\"\\nâœ“ Fold {fold_idx} í•™ìŠµ ì™„ë£Œ! ìµœì € ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'fold': fold_idx,\n",
        "        'history': history,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'best_val_acc': max(history['val_acc']),\n",
        "        'model_path': best_model_path\n",
        "    }\n",
        "\n",
        "# --- 7. ë©”ì¸ ì‹¤í–‰ (Fold 2ë§Œ ì‹¤í–‰) ---\n",
        "all_fold_results = []\n",
        "try:\n",
        "    # (íŒ€ì›ë¶„ì˜ ì „ëµëŒ€ë¡œ Fold 2 (ì¸ë±ìŠ¤ 1)ë§Œ ìš°ì„  ì‹¤í–‰)\n",
        "    fold_info = fold_splits[0]\n",
        "    fold_idx = fold_info['fold']\n",
        "    train_drivers = fold_info['train_drivers']\n",
        "    val_drivers = fold_info['val_drivers']\n",
        "\n",
        "    fold_result = train_fold(fold_idx, train_drivers, val_drivers)\n",
        "    all_fold_results.append(fold_result)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n--- ìµœì¢… ê²°ê³¼ (Fold {fold_result['fold']}) ---\")\n",
        "    print(f\"  ìµœì € Val Loss: {fold_result['best_val_loss']:.4f}\")\n",
        "    print(f\"  ìµœê³  Val Acc: {fold_result['best_val_acc']:.2f}%\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nğŸš¨ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\n",
        "\n",
        "# --- 8. ë‹¨ì¼ ëª¨ë¸ ì˜ˆì¸¡ ë° ì œì¶œ ---\n",
        "if not all_fold_results:\n",
        "    print(\"ğŸš¨ ì˜¤ë¥˜: í•™ìŠµëœ í´ë“œ ê²°ê³¼ê°€ ì—†ì–´ ì˜ˆì¸¡ì„ ìƒëµí•©ë‹ˆë‹¤.\")\n",
        "else:\n",
        "    result = all_fold_results[0] # (Fold 2ì˜ ê²°ê³¼)\n",
        "    fold_idx = result['fold']\n",
        "    model_path = result['model_path']\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"ğŸ”® Fold {fold_idx} ë‹¨ì¼ ëª¨ë¸ ì˜ˆì¸¡ ì‹œì‘\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    test_dataset = DriverDataset(\n",
        "        test_dir, driver_df, [],\n",
        "        transform=team_transform_eval, is_test=True\n",
        "    )\n",
        "\n",
        "    def collate_fn_test(batch):\n",
        "        batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "        if not batch: return torch.Tensor(), []\n",
        "        images, filenames = zip(*batch)\n",
        "        return torch.stack(images), list(filenames)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, collate_fn=collate_fn_test\n",
        "    )\n",
        "\n",
        "    print(f\"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_dataset)}ê°œ\")\n",
        "\n",
        "    # --- [ëª¨ë¸ ìˆ˜ì •] ResNet-50 ë¼ˆëŒ€ ìƒì„± ---\n",
        "    model = timm.create_model('resnet50', pretrained=False, num_classes=num_classes)\n",
        "\n",
        "    print(f\"\\nğŸ“ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}...\")\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"âœ“ Fold {fold_idx} ëª¨ë¸ (Epoch {checkpoint['epoch']+1}, Val Loss: {checkpoint.get('val_loss', 'N/A'):.4f}) ë¡œë“œ ì™„ë£Œ.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "        raise # (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ì§€)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    img_names = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, filenames in tqdm(test_loader, desc=f'Fold {fold_idx} ì˜ˆì¸¡'):\n",
        "            if images.nelement() == 0: continue # (ë¹ˆ ë°°ì¹˜ ìŠ¤í‚µ)\n",
        "\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            predictions.append(probs.cpu().numpy())\n",
        "            img_names.extend(filenames)\n",
        "\n",
        "    final_predictions = np.vstack(predictions)\n",
        "    print(f\"\\nâœ“ ì˜ˆì¸¡ ì™„ë£Œ: {final_predictions.shape}\")\n",
        "\n",
        "    # --- Submission íŒŒì¼ ìƒì„± ---\n",
        "    class_cols = [f'c{i}' for i in range(num_classes)]\n",
        "    submission_data = {'img': img_names}\n",
        "    for i, col in enumerate(class_cols):\n",
        "        submission_data[col] = final_predictions[:, i]\n",
        "\n",
        "    submission = pd.DataFrame(submission_data)\n",
        "\n",
        "    # [ìˆ˜ì •] íŒŒì¼ ì´ë¦„ ë³€ê²½\n",
        "    submission_file = f'resnet50_fold{fold_idx}_single_model_submission.csv'\n",
        "    submission.to_csv(submission_file, index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"âœ… Submission íŒŒì¼ ìƒì„± ì™„ë£Œ: {submission_file}\")\n",
        "    print(f\"âœ… ì´ {len(submission)}ê°œ ì´ë¯¸ì§€ ì˜ˆì¸¡\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nğŸ“‹ Submission ìƒ˜í”Œ:\")\n",
        "    print(submission.head())"
      ],
      "metadata": {
        "id": "q7ajhsaQ8pLp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}