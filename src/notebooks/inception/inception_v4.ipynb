{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8663213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Using cached protobuf-6.33.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting typing_extensions>=3.6.6 (from tensorflow)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-2.0.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from tensorflow) (2.3.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.3-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Using cached optree-0.17.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached markupsafe-3.0.3-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl (331.9 MB)\n",
      "Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl (4.7 MB)\n",
      "   ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 2.4/4.7 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.7/4.7 MB 16.7 MB/s  0:00:00\n",
      "Using cached ml_dtypes-0.5.3-cp312-cp312-win_amd64.whl (208 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp312-cp312-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 27.8 MB/s  0:00:00\n",
      "Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 39.1 MB/s  0:00:00\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached protobuf-6.33.0-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached markupsafe-3.0.3-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading wrapt-2.0.0-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached optree-0.17.0-cp312-cp312-win_amd64.whl (314 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, typing_extensions, termcolor, tensorboard-data-server, setuptools, protobuf, opt_einsum, ml_dtypes, mdurl, MarkupSafe, markdown, idna, h5py, google_pasta, gast, charset_normalizer, certifi, absl-py, werkzeug, requests, optree, markdown-it-py, grpcio, astunparse, tensorboard, rich, keras, tensorflow\n",
      "\n",
      "   - --------------------------------------  1/33 [libclang]\n",
      "   - --------------------------------------  1/33 [libclang]\n",
      "   ------ ---------------------------------  5/33 [urllib3]\n",
      "   ---------- -----------------------------  9/33 [setuptools]\n",
      "   ---------- -----------------------------  9/33 [setuptools]\n",
      "   ---------- -----------------------------  9/33 [setuptools]\n",
      "   ---------- -----------------------------  9/33 [setuptools]\n",
      "   ---------- -----------------------------  9/33 [setuptools]\n",
      "   ------------ --------------------------- 10/33 [protobuf]\n",
      "   ------------- -------------------------- 11/33 [opt_einsum]\n",
      "   ------------------ --------------------- 15/33 [markdown]\n",
      "   -------------------- ------------------- 17/33 [h5py]\n",
      "   ----------------------- ---------------- 19/33 [gast]\n",
      "   --------------------------- ------------ 23/33 [werkzeug]\n",
      "   ----------------------------- ---------- 24/33 [requests]\n",
      "   ------------------------------- -------- 26/33 [markdown-it-py]\n",
      "   -------------------------------- ------- 27/33 [grpcio]\n",
      "   ----------------------------------- ---- 29/33 [tensorboard]\n",
      "   ----------------------------------- ---- 29/33 [tensorboard]\n",
      "   ----------------------------------- ---- 29/33 [tensorboard]\n",
      "   ------------------------------------ --- 30/33 [rich]\n",
      "   ------------------------------------- -- 31/33 [keras]\n",
      "   ------------------------------------- -- 31/33 [keras]\n",
      "   ------------------------------------- -- 31/33 [keras]\n",
      "   ------------------------------------- -- 31/33 [keras]\n",
      "   ------------------------------------- -- 31/33 [keras]\n",
      "   ------------------------------------- -- 31/33 [keras]\n",
      "   ------------------------------------- -- 31/33 [keras]\n",
      "   ------------------------------------- -- 31/33 [keras]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   -------------------------------------- - 32/33 [tensorflow]\n",
      "   ---------------------------------------- 33/33 [tensorflow]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.3 absl-py-2.3.1 astunparse-1.6.3 certifi-2025.10.5 charset_normalizer-3.4.4 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 idna-3.11 keras-3.12.0 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 protobuf-6.33.0 requests-2.32.5 rich-14.2.0 setuptools-80.9.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 typing_extensions-4.15.0 urllib3-2.5.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-2.0.0\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\pycharmprojects\\deeplearning-term-proj\\.venv\\lib\\site-packages (12.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install pandas numpy matplotlib\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168a501",
   "metadata": {},
   "source": [
    "# Inception V4 모델 구현\n",
    "\n",
    "이 노트북에서는 운전자 행동 분류를 위한 Inception V4 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e02f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception V4 모듈 구현\n",
    "\n",
    "def conv2d_bn(x, filters, kernel_size, strides=1, padding='same', activation='relu', name=None):\n",
    "    \"\"\"Convolution + Batch Normalization + Activation\"\"\"\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, \n",
    "                      use_bias=False, name=name)(x)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def inception_stem(input_tensor):\n",
    "    \"\"\"Inception V4의 Stem 모듈\"\"\"\n",
    "    x = conv2d_bn(input_tensor, 32, 3, strides=2, padding='valid')\n",
    "    x = conv2d_bn(x, 32, 3, padding='valid')\n",
    "    x = conv2d_bn(x, 64, 3)\n",
    "    \n",
    "    branch_0 = layers.MaxPooling2D(3, strides=2, padding='valid')(x)\n",
    "    branch_1 = conv2d_bn(x, 96, 3, strides=2, padding='valid')\n",
    "    x = layers.concatenate([branch_0, branch_1], axis=-1)\n",
    "    \n",
    "    branch_0 = conv2d_bn(x, 64, 1)\n",
    "    branch_0 = conv2d_bn(branch_0, 96, 3, padding='valid')\n",
    "    \n",
    "    branch_1 = conv2d_bn(x, 64, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 64, (7, 1))\n",
    "    branch_1 = conv2d_bn(branch_1, 64, (1, 7))\n",
    "    branch_1 = conv2d_bn(branch_1, 96, 3, padding='valid')\n",
    "    \n",
    "    x = layers.concatenate([branch_0, branch_1], axis=-1)\n",
    "    \n",
    "    branch_0 = conv2d_bn(x, 192, 3, strides=2, padding='valid')\n",
    "    branch_1 = layers.MaxPooling2D(3, strides=2, padding='valid')(x)\n",
    "    x = layers.concatenate([branch_0, branch_1], axis=-1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def inception_a(input_tensor):\n",
    "    \"\"\"Inception-A 모듈\"\"\"\n",
    "    branch_0 = conv2d_bn(input_tensor, 96, 1)\n",
    "    \n",
    "    branch_1 = conv2d_bn(input_tensor, 64, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 96, 3)\n",
    "    \n",
    "    branch_2 = conv2d_bn(input_tensor, 64, 1)\n",
    "    branch_2 = conv2d_bn(branch_2, 96, 3)\n",
    "    branch_2 = conv2d_bn(branch_2, 96, 3)\n",
    "    \n",
    "    branch_3 = layers.AveragePooling2D(3, strides=1, padding='same')(input_tensor)\n",
    "    branch_3 = conv2d_bn(branch_3, 96, 1)\n",
    "    \n",
    "    x = layers.concatenate([branch_0, branch_1, branch_2, branch_3], axis=-1)\n",
    "    return x\n",
    "\n",
    "def inception_b(input_tensor):\n",
    "    \"\"\"Inception-B 모듈\"\"\"\n",
    "    branch_0 = conv2d_bn(input_tensor, 384, 1)\n",
    "    \n",
    "    branch_1 = conv2d_bn(input_tensor, 192, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 224, (1, 7))\n",
    "    branch_1 = conv2d_bn(branch_1, 256, (7, 1))\n",
    "    \n",
    "    branch_2 = conv2d_bn(input_tensor, 192, 1)\n",
    "    branch_2 = conv2d_bn(branch_2, 192, (1, 7))\n",
    "    branch_2 = conv2d_bn(branch_2, 224, (7, 1))\n",
    "    branch_2 = conv2d_bn(branch_2, 224, (1, 7))\n",
    "    branch_2 = conv2d_bn(branch_2, 256, (7, 1))\n",
    "    \n",
    "    branch_3 = layers.AveragePooling2D(3, strides=1, padding='same')(input_tensor)\n",
    "    branch_3 = conv2d_bn(branch_3, 128, 1)\n",
    "    \n",
    "    x = layers.concatenate([branch_0, branch_1, branch_2, branch_3], axis=-1)\n",
    "    return x\n",
    "\n",
    "def inception_c(input_tensor):\n",
    "    \"\"\"Inception-C 모듈\"\"\"\n",
    "    branch_0 = conv2d_bn(input_tensor, 256, 1)\n",
    "    \n",
    "    branch_1 = conv2d_bn(input_tensor, 384, 1)\n",
    "    branch_1_0 = conv2d_bn(branch_1, 256, (1, 3))\n",
    "    branch_1_1 = conv2d_bn(branch_1, 256, (3, 1))\n",
    "    branch_1 = layers.concatenate([branch_1_0, branch_1_1], axis=-1)\n",
    "    \n",
    "    branch_2 = conv2d_bn(input_tensor, 384, 1)\n",
    "    branch_2 = conv2d_bn(branch_2, 448, (1, 3))\n",
    "    branch_2 = conv2d_bn(branch_2, 512, (3, 1))\n",
    "    branch_2_0 = conv2d_bn(branch_2, 256, (3, 1))\n",
    "    branch_2_1 = conv2d_bn(branch_2, 256, (1, 3))\n",
    "    branch_2 = layers.concatenate([branch_2_0, branch_2_1], axis=-1)\n",
    "    \n",
    "    branch_3 = layers.AveragePooling2D(3, strides=1, padding='same')(input_tensor)\n",
    "    branch_3 = conv2d_bn(branch_3, 256, 1)\n",
    "    \n",
    "    x = layers.concatenate([branch_0, branch_1, branch_2, branch_3], axis=-1)\n",
    "    return x\n",
    "\n",
    "def reduction_a(input_tensor):\n",
    "    \"\"\"Reduction-A 모듈 (Inception-A에서 Inception-B로 전환)\"\"\"\n",
    "    branch_0 = conv2d_bn(input_tensor, 384, 3, strides=2, padding='valid')\n",
    "    \n",
    "    branch_1 = conv2d_bn(input_tensor, 192, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 224, 3)\n",
    "    branch_1 = conv2d_bn(branch_1, 256, 3, strides=2, padding='valid')\n",
    "    \n",
    "    branch_2 = layers.MaxPooling2D(3, strides=2, padding='valid')(input_tensor)\n",
    "    \n",
    "    x = layers.concatenate([branch_0, branch_1, branch_2], axis=-1)\n",
    "    return x\n",
    "\n",
    "def reduction_b(input_tensor):\n",
    "    \"\"\"Reduction-B 모듈 (Inception-B에서 Inception-C로 전환)\"\"\"\n",
    "    branch_0 = conv2d_bn(input_tensor, 192, 1)\n",
    "    branch_0 = conv2d_bn(branch_0, 192, 3, strides=2, padding='valid')\n",
    "    \n",
    "    branch_1 = conv2d_bn(input_tensor, 256, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 256, (1, 7))\n",
    "    branch_1 = conv2d_bn(branch_1, 320, (7, 1))\n",
    "    branch_1 = conv2d_bn(branch_1, 320, 3, strides=2, padding='valid')\n",
    "    \n",
    "    branch_2 = layers.MaxPooling2D(3, strides=2, padding='valid')(input_tensor)\n",
    "    \n",
    "    x = layers.concatenate([branch_0, branch_1, branch_2], axis=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcac51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inception_v4(input_shape=(299, 299, 3), num_classes=10):\n",
    "    \"\"\"전체 Inception V4 모델 생성\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Stem\n",
    "    x = inception_stem(inputs)\n",
    "    \n",
    "    # 4 x Inception-A\n",
    "    for _ in range(4):\n",
    "        x = inception_a(x)\n",
    "    \n",
    "    # Reduction-A\n",
    "    x = reduction_a(x)\n",
    "    \n",
    "    # 7 x Inception-B\n",
    "    for _ in range(7):\n",
    "        x = inception_b(x)\n",
    "    \n",
    "    # Reduction-B\n",
    "    x = reduction_b(x)\n",
    "    \n",
    "    # 3 x Inception-C\n",
    "    for _ in range(3):\n",
    "        x = inception_c(x)\n",
    "    \n",
    "    # Final pooling and prediction\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='inception_v4')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = create_inception_v4(input_shape=(img_height, img_width, 3), num_classes=num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055966bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강 설정\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 20%를 검증 데이터로 사용\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 학습 데이터 로더\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# 검증 데이터 로더\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "print(f\"학습 샘플 수: {train_generator.samples}\")\n",
    "print(f\"검증 샘플 수: {validation_generator.samples}\")\n",
    "print(f\"클래스: {train_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 콜백 설정\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_inception_v4_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8216fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 시각화\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Accuracy 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 예측 (submission 파일 생성)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 예측\n",
    "predictions = model.predict(test_generator, verbose=1)\n",
    "\n",
    "# 제출 파일 생성\n",
    "test_filenames = test_generator.filenames\n",
    "test_filenames = [os.path.basename(f) for f in test_filenames]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'img': test_filenames,\n",
    "    'c0': predictions[:, 0],\n",
    "    'c1': predictions[:, 1],\n",
    "    'c2': predictions[:, 2],\n",
    "    'c3': predictions[:, 3],\n",
    "    'c4': predictions[:, 4],\n",
    "    'c5': predictions[:, 5],\n",
    "    'c6': predictions[:, 6],\n",
    "    'c7': predictions[:, 7],\n",
    "    'c8': predictions[:, 8],\n",
    "    'c9': predictions[:, 9]\n",
    "})\n",
    "\n",
    "submission.to_csv('inception_v4_submission.csv', index=False)\n",
    "print(\"제출 파일이 생성되었습니다: inception_v4_submission.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d71a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('inception_v4_final_model.h5')\n",
    "print(\"최종 모델이 저장되었습니다: inception_v4_final_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정\n",
    "base_dir = r'c:\\Users\\USER\\PycharmProjects\\DeepLearning-Term-Proj'\n",
    "train_dir = os.path.join(base_dir, 'data', 'imgs', 'train')\n",
    "test_dir = os.path.join(base_dir, 'data', 'imgs', 'test')\n",
    "\n",
    "# 이미지 파라미터\n",
    "img_height, img_width = 299, 299  # Inception V4는 299x299 입력 사이즈 사용\n",
    "batch_size = 32\n",
    "num_classes = 10  # c0 ~ c9 (10개 클래스)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
