{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42258a5e",
   "metadata": {},
   "source": [
    "# Resnet 5.\n",
    "resnet 4_2 -> 50dÎ°ú Î™®Îç∏Îßå ÌôïÏû•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b5c48",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# GPU ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'ÏÇ¨Ïö© ÎîîÎ∞îÏù¥Ïä§: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b485182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ============================================================\n",
    "# Ïû¨ÌòÑÏÑ± Î≥¥Ïû• (Reproducibility)\n",
    "# ============================================================\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Î™®Îì† ÎÇúÏàò ÏÉùÏÑ±Í∏∞Ïùò ÏãúÎìúÎ•º Í≥†Ï†ïÌïòÏó¨ Ïû¨ÌòÑÏÑ± Î≥¥Ïû•\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # multi-GPU\n",
    "    \n",
    "    # CUDA Í≤∞Ï†ïÎ°†Ï†Å ÎèôÏûë ÌôúÏÑ±Ìôî\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # PyTorch Í≤∞Ï†ïÎ°†Ï†Å ÏïåÍ≥†Î¶¨Ï¶ò ÏÇ¨Ïö© (PyTorch 1.8+)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    \n",
    "    print(f\"‚úÖ Random Seed Í≥†Ï†ï ÏôÑÎ£å: {seed}\")\n",
    "    print(f\"   - Python random: {seed}\")\n",
    "    print(f\"   - NumPy: {seed}\")\n",
    "    print(f\"   - PyTorch: {seed}\")\n",
    "    print(f\"   - CUDA: {seed}\")\n",
    "    print(f\"   - cuDNN Deterministic: True\")\n",
    "    print(f\"   - cuDNN Benchmark: False\")\n",
    "    print(f\"   - PyTorch Deterministic Algorithms: True\")\n",
    "\n",
    "# Seed Ï†ÅÏö©\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a066304",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707366a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
    "img_size = 256\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "num_epochs = 50\n",
    "# learning_rate = 0.0003 -> train section\n",
    "num_workers = 0\n",
    "version = \"5\"\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú\n",
    "base_dir = r'c:\\Users\\USER\\PycharmProjects\\DeepLearning-Term-Proj'\n",
    "driver_csv_path = os.path.join(base_dir, 'data', 'driver_imgs_list.csv')\n",
    "train_dir = os.path.join(base_dir, 'data', 'imgs', 'train')\n",
    "test_dir = os.path.join(base_dir, 'data', 'imgs', 'test')\n",
    "name = f'resnet_{version}'\n",
    "\n",
    "print(f\"Train directory: {train_dir}\")\n",
    "print(f\"Test directory: {test_dir}\")\n",
    "print(f\"Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞: {img_size}x{img_size}\")\n",
    "print(f\"Î∞∞Ïπò ÌÅ¨Í∏∞: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a060b1",
   "metadata": {},
   "source": [
    "## offline prepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def apply_clahe_to_folder(src_root, dst_root, clip=2.0, tile=8, is_test=False):\n",
    "#     os.makedirs(dst_root, exist_ok=True)\n",
    "    \n",
    "#     if is_test:\n",
    "#         process_images(src_root, dst_root, clip, tile, desc=\"Test Images\")\n",
    "#         return\n",
    "    \n",
    "#     for label in sorted(os.listdir(src_root)):\n",
    "#         src_dir = os.path.join(src_root, label)\n",
    "#         dst_dir = os.path.join(dst_root, label)\n",
    "#         os.makedirs(dst_dir, exist_ok=True)\n",
    "        \n",
    "#         process_images(src_dir, dst_dir, clip, tile, desc=f\"Class {label}\")\n",
    "\n",
    "\n",
    "# def process_images(src_dir, dst_dir, clip, tile, desc=\"Processing\"):\n",
    "#     \"\"\"Í∞úÎ≥Ñ Ïù¥ÎØ∏ÏßÄÏóê CLAHE Ï†ÅÏö©\"\"\"\n",
    "#     img_files = [f for f in os.listdir(src_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "#     for img_name in tqdm(sorted(img_files), desc=desc):\n",
    "#         src_path = os.path.join(src_dir, img_name)\n",
    "#         dst_path = os.path.join(dst_dir, img_name)\n",
    "        \n",
    "#         if os.path.exists(dst_path):\n",
    "#             continue\n",
    "        \n",
    "#         img = cv2.imread(src_path)\n",
    "#         if img is None:\n",
    "#             print(f\"‚ö†Ô∏è ÏùΩÍ∏∞ Ïã§Ìå®: {src_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "#         l, a, b = cv2.split(img_lab)\n",
    "        \n",
    "#         clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(tile, tile))\n",
    "#         l_clahe = clahe.apply(l)\n",
    "        \n",
    "#         img_clahe = cv2.merge([l_clahe, a, b])\n",
    "#         img_clahe = cv2.cvtColor(img_clahe, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "#         cv2.imwrite(dst_path, img_clahe)\n",
    "\n",
    "# apply_clahe_to_folder(\n",
    "#     src_root=os.path.join(base_dir, 'data', 'imgs', 'train'),\n",
    "#     dst_root=os.path.join(base_dir, 'data', 'clahe_imgs', 'train'),\n",
    "#     clip=2.0,\n",
    "#     tile=8\n",
    "# )\n",
    "\n",
    "# apply_clahe_to_folder(\n",
    "#     src_root=os.path.join(base_dir, 'data', 'imgs', 'test'),\n",
    "#     dst_root=os.path.join(base_dir, 'data', 'clahe_imgs', 'test'),\n",
    "#     clip=2.0,\n",
    "#     tile=8,\n",
    "# \tis_test = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe_train_dir = os.path.join(base_dir, 'data', 'clahe_imgs', 'train')\n",
    "clahe_test_dir = os.path.join(base_dir, 'data', 'clahe_imgs', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97239b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_df = pd.read_csv(driver_csv_path)\n",
    "\n",
    "print(f\"Í≥†Ïú† Ïö¥Ï†ÑÏûê Ïàò: {driver_df['subject'].nunique()}Î™Ö\")\n",
    "print(f\"Ïö¥Ï†ÑÏûê Î™©Î°ù: {sorted(driver_df['subject'].unique())}\")\n",
    "\n",
    "# driver_counts = driver_df['subject'].value_counts().sort_index()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.bar(driver_counts.index.astype(str), driver_counts.values, color='C0', alpha=0.9)\n",
    "# plt.xlabel('Driver')\n",
    "# plt.ylabel('Image Count')\n",
    "# plt.title('Images per Driver')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.grid(axis='y', alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10c8b8",
   "metadata": {},
   "source": [
    "# 5-Fold Cross Validation\n",
    "foldÎ≥Ñ Ïö¥Ï†ÑÏûê Î™©Î°ùÏùÑ ÎØ∏Î¶¨ ÎÇòÎàÑÍ∏¥ ÌïòÎêò Ï¥àÍ∏∞ Ïã§Ìóò Îã®Í≥ÑÏóêÏÑúÎäî Ìïú Ìè¥Îìú(fold 2, ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Í∞Ä Í∞ÄÏû• ÎßéÏïÑÏÑú..)Îßå ÏÇ¨Ïö©ÌïòÍ≥† Ïù¥ÌõÑ ÎßàÎ¨¥Î¶¨ Îã®Í≥ÑÏóêÏÑú Ï†ÑÏ≤¥ Ìè¥Îìú Îã§ ÎèåÎ†§ÏÑú ÏùºÎ∞òÌôî ÏÑ±Îä• ÎÅåÏñ¥Ïò¨Î¶¨Îäî Î∞©Ìñ•ÏúºÎ°ú ÏßÑÌñâ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa684f",
   "metadata": {},
   "source": [
    "## split train data into 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drivers = sorted(driver_df['subject'].unique())\n",
    "\n",
    "n_folds = 5\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold_splits = []\n",
    "for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(all_drivers)):\n",
    "\ttrain_drivers = [all_drivers[i] for i in train_indices]\n",
    "\tval_drivers = [all_drivers[i] for i in val_indices]\n",
    "\n",
    "\tfold_splits.append({\n",
    "\t\t'fold': fold_idx+1,\n",
    "\t\t'train_drivers': train_drivers,\n",
    "\t\t'val_drivers': val_drivers\n",
    "\t})\n",
    "\n",
    "\tprint(\"Fold\", fold_idx+1)\n",
    "\tprint(\"train:\", train_drivers, \"val:\", val_drivers)\n",
    "\t\n",
    "\ttrain_imgs = driver_df[driver_df['subject'].isin(train_drivers)]\n",
    "\tval_imgs = driver_df[driver_df['subject'].isin(val_drivers)]\n",
    "\tprint(f\"ÌïôÏäµ Ïù¥ÎØ∏ÏßÄ: {len(train_imgs)}Í∞ú\")\n",
    "\tprint(f\"Í≤ÄÏ¶ù Ïù¥ÎØ∏ÏßÄ: {len(val_imgs)}Í∞ú\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ddc81",
   "metadata": {},
   "source": [
    "## define DraiverDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriverDataset(Dataset):\n",
    "    \"\"\"Ïö¥Ï†ÑÏûê ÌñâÎèô Îç∞Ïù¥ÌÑ∞ÏÖã\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, driver_df, driver_list, transform=None, is_test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        if is_test:\n",
    "            test_images_dir = data_dir\n",
    "            for img_name in os.listdir(test_images_dir):\n",
    "                self.images.append(os.path.join(test_images_dir, img_name))\n",
    "        else: #is_train\n",
    "            driver_subset = driver_df[driver_df['subject'].isin(driver_list)]\n",
    "\n",
    "            for _, row in driver_subset.iterrows():\n",
    "                class_name = row['classname']\n",
    "                img_name = row['img']\n",
    "                img_path = os.path.join(data_dir, class_name, img_name)\n",
    "\n",
    "                self.images.append(img_path)\n",
    "                class_idx = int(class_name[1:])\n",
    "                self.labels.append(class_idx)\n",
    "        print(f\"{'ÌÖåÏä§Ìä∏' if is_test else 'Ïö¥Ï†ÑÏûê' + str(len(driver_list))+'Î™Ö'}, Îç∞Ïù¥ÌÑ∞ {len(self.images)}Í∞ú Ïù¥ÎØ∏ÏßÄ\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return image, os.path.basename(img_path)\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8b549",
   "metadata": {},
   "source": [
    "## define online transform\n",
    "baseline Í∏∞Î≥∏ ÏÑ±Îä•ÏùÑ ÌôïÏù∏ÌïòÍ≥†Ïûê resize, to tensor, normalizeÎßå Ï†ÅÏö©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de85129",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "\ttransforms.Resize((img_size, img_size)),\n",
    "\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "\ttransforms.Resize((img_size, img_size)),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451010f0",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb822b",
   "metadata": {},
   "source": [
    "### define custom resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "EMBED_DIM = 512\n",
    "DROPOUT = 0.4\n",
    "DROP_PATH = 0.1\n",
    "EARLY_STOP_PATIENCE = 10 \n",
    "\n",
    "class ResNetDWithHead(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim=512, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            'resnet50d',\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool='',\n",
    "            drop_path_rate=DROP_PATH,\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        in_ch = self.backbone.num_features\n",
    "\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, in_ch, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(in_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(in_ch, embed_dim),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward_features_map(self, x):\n",
    "        return self.backbone.forward_features(x)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x = self.forward_features_map(x)\n",
    "        x = self.embed(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b055b",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f55cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"DataLoader workerÏùò Ïû¨ÌòÑÏÑ± Î≥¥Ïû•\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee783da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(fold_idx, train_drivers, val_drivers):\n",
    "    print(f\"==== Fold {fold_idx}/{n_folds} ====\")\n",
    "\n",
    "    # ===== Îç∞Ïù¥ÌÑ∞ÏÖã & Î°úÎçî =====\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "\n",
    "    train_dataset = DriverDataset(\n",
    "        clahe_train_dir, driver_df, train_drivers,\n",
    "        transform=transform_train, is_test=False\n",
    "    )\n",
    "    val_dataset = DriverDataset(\n",
    "        clahe_train_dir, driver_df, val_drivers,\n",
    "        transform=transform_eval, is_test=False\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=True,\n",
    "                              worker_init_fn=worker_init_fn, generator=g)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=True,\n",
    "                            worker_init_fn=worker_init_fn, generator=g)\n",
    "\n",
    "    print(f\"ÌïôÏäµ Î∞∞Ïπò Ïàò: {len(train_loader)}\")\n",
    "    print(f\"Í≤ÄÏ¶ù Î∞∞Ïπò Ïàò: {len(val_loader)}\")\n",
    "\n",
    "    # ===== Î™®Îç∏ =====\n",
    "    model = ResNetDWithHead(\n",
    "        num_classes=num_classes,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        dropout=DROPOUT,\n",
    "    ).to(device)\n",
    "\n",
    "    # ---- layer1ÍπåÏßÄ ÎèôÍ≤∞ (stem~layer1 freeze) ----\n",
    "    for n, p in model.backbone.named_parameters():\n",
    "        if n.startswith('layer2') or n.startswith('layer3') or n.startswith('layer4'):\n",
    "            p.requires_grad = True\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "    # headÎäî ÌïôÏäµ\n",
    "    for p in model.embed.parameters(): p.requires_grad = True\n",
    "    for p in model.proj.parameters(): p.requires_grad = True\n",
    "    for p in model.classifier.parameters(): p.requires_grad = True\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ ÌÜµÍ≥Ñ\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞:      {total_params:>15,}\")\n",
    "    print(f\"  ÌïôÏäµ Í∞ÄÎä• ÌååÎùºÎØ∏ÌÑ∞:  {trainable_params:>15,} ({100*trainable_params/total_params:>6.2f}%)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # ===== ÏÜêÏã§/ÏòµÌã∞Îßà/Ïä§ÏºÄÏ§ÑÎü¨ =====\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=3e-4,\n",
    "        weight_decay=5e-2\n",
    "    )\n",
    "    warmup_epochs = 5\n",
    "    cosine_epochs = max(1, num_epochs - warmup_epochs)\n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[\n",
    "            LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_epochs),\n",
    "            CosineAnnealingLR(optimizer, T_max=cosine_epochs)\n",
    "        ],\n",
    "        milestones=[warmup_epochs]\n",
    "    )\n",
    "\n",
    "    # ===== Í∏∞Î°ù =====\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],\n",
    "               'val_macro_f1': [], 'val_logloss': [], 'learning_rates': []}\n",
    "\n",
    "    # ===== Early Stopping =====\n",
    "    early_stop_patience = EARLY_STOP_PATIENCE\n",
    "    patience_counter = 0\n",
    "    best_metric = float('inf')\n",
    "    best_model_path = f'models/{name}.pth'\n",
    "\n",
    "    print(f\"\\n‚è±Ô∏è Early Stopping Patience (val logloss Í∏∞Ï§Ä): {early_stop_patience} ep\\n\")\n",
    "    print(\"=\"*70); print(\"üöÄ ÌïôÏäµ ÏãúÏûë\"); print(\"=\"*70)\n",
    "\n",
    "    def eval_on_loader(model, loader, criterion):\n",
    "        model.eval()\n",
    "        total, correct, running_loss = 0, 0, 0.0\n",
    "        all_probs, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(loader, desc='Validating', leave=False)\n",
    "            for images, labels in pbar:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                logits = model(images)\n",
    "\n",
    "                loss = criterion(logits, labels)\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                probs = logits.softmax(dim=1).detach().cpu().numpy()\n",
    "                all_probs.append(probs)\n",
    "                all_labels.append(labels.detach().cpu().numpy())\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}',\n",
    "                                  'acc': f'{100.0*correct/max(1,total):.2f}%'})\n",
    "\n",
    "        avg_loss = running_loss / max(1, total)\n",
    "        acc = 100.0 * correct / max(1, total)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        macro_f1 = f1_score(all_labels, np.argmax(all_probs, axis=1), average='macro')\n",
    "        mlogloss = log_loss(all_labels, all_probs, labels=list(range(num_classes)))\n",
    "        return avg_loss, acc, macro_f1, mlogloss\n",
    "\n",
    "    # ===== ÏóêÌè≠ Î£®ÌîÑ =====\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\n{\"=\"*70}')\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'{\"=\"*70}')\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['learning_rates'].append(current_lr)\n",
    "\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        pbar = tqdm(train_loader, desc='Training', leave=False)\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(images)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * labels.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}',\n",
    "                              'acc': f'{100.0*train_correct/max(1,train_total):.2f}%',\n",
    "                              'lr': f'{current_lr:.6f}'})\n",
    "\n",
    "        epoch_train_loss = train_loss / max(1, train_total)\n",
    "        epoch_train_acc = 100.0 * train_correct / max(1, train_total)\n",
    "\n",
    "        # ---- Validate ----\n",
    "        val_loss, val_acc, val_macro_f1, val_logloss = eval_on_loader(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_macro_f1'].append(val_macro_f1)\n",
    "        history['val_logloss'].append(val_logloss)\n",
    "\n",
    "        print(f'\\nüìä Epoch {epoch+1} Í≤∞Í≥º:')\n",
    "        print(f'  Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.2f}%')\n",
    "        print(f'  Val   Loss: {val_loss:.4f} | Val Acc:   {val_acc:.2f}%')\n",
    "        print(f'  Val Macro-F1: {val_macro_f1:.4f} | Val LogLoss: {val_logloss:.4f}')\n",
    "        print(f'  LR: {current_lr:.6f}')\n",
    "\n",
    "        # ---- Best / Early Stop ----\n",
    "        if val_logloss < best_metric:\n",
    "            best_metric = val_logloss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'fold': fold_idx,\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'val_macro_f1': val_macro_f1,\n",
    "                'val_logloss': val_logloss,\n",
    "                'model_name': name,\n",
    "                'freeze_mode': 'freeze_to_layer1',\n",
    "                'drop_path_rate': DROP_PATH,\n",
    "                'trainable_params': trainable_params,\n",
    "                'total_params': total_params,\n",
    "                'seed': SEED\n",
    "            }, best_model_path)\n",
    "            print(f'  ‚úÖ ÏµúÍ≥† ÏÑ±Îä•(val_logloss) Î™®Îç∏ Ï†ÄÏû•! (val_logloss: {val_logloss:.4f})')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  ‚è≥ Early Stopping Ïπ¥Ïö¥ÌÑ∞: {patience_counter}/{EARLY_STOP_PATIENCE}')\n",
    "            if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "                print(f'\\n{\"=\"*70}')\n",
    "                print(f'üõë Early Stopping Î∞úÎèô! (Epoch {epoch+1}) ‚Äî val_logloss Í∞úÏÑ† ÏóÜÏùå')\n",
    "                print(f'   ÏµúÍ≥† val_logloss: {best_metric:.4f}')\n",
    "                print(f'   Î™®Îç∏ Ï†ÄÏû• Í≤ΩÎ°ú: {best_model_path}')\n",
    "                print(f'{\"=\"*70}')\n",
    "                break\n",
    "\n",
    "    final_epoch = epoch + 1\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"‚úÖ Fold {fold_idx} ÌïôÏäµ ÏôÑÎ£å! (EXP-1+)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  Ï¥ù ÌïôÏäµ ÏóêÌè≠: {final_epoch}/{num_epochs}\")\n",
    "    print(f\"  ÏµúÍ≥† Macro-F1: {max(history['val_macro_f1']):.4f}\")\n",
    "    print(f\"  ÏµúÏ†Ä Val Loss: {min(history['val_loss']):.4f}\")\n",
    "    print(f\"  ÏµúÏ†Ä LogLoss:  {min(history['val_logloss']):.4f}\")\n",
    "    print(f\"  ÏµúÍ≥† Val Acc:  {max(history['val_acc']):.2f}%\")\n",
    "    print(f\"  Î™®Îç∏ Ï†ÄÏû•: {best_model_path}\")\n",
    "    print(f\"  ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return {\n",
    "        'fold': fold_idx,\n",
    "        'history': history,\n",
    "        'best_macro_f1': max(history['val_macro_f1']),\n",
    "        'best_val_loss': min(history['val_loss']),\n",
    "        'best_val_logloss': min(history['val_logloss']),\n",
    "        'best_val_acc': max(history['val_acc']),\n",
    "        'model_path': best_model_path,\n",
    "        'stopped_epoch': final_epoch,\n",
    "        'model_name': name,\n",
    "        'freeze_mode': 'freeze_to_layer1',\n",
    "        'trainable_params': trainable_params,\n",
    "        'total_params': total_params,\n",
    "        'seed':SEED\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Fold 2 ÌïôÏäµ (Inception-CÎ∂ÄÌÑ∞ Fine-tuning) ==========\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "fold_info = fold_splits[1]  # Fold 2\n",
    "fold_idx = fold_info['fold']\n",
    "train_drivers = fold_info['train_drivers']\n",
    "val_drivers = fold_info['val_drivers']\n",
    "\n",
    "\n",
    "print(f\"==== {name} ====\")\n",
    "\n",
    "\n",
    "fold_result = train_fold(\n",
    "    fold_idx, \n",
    "    train_drivers, \n",
    "    val_drivers,\n",
    ")\n",
    "\n",
    "all_fold_results.append(fold_result)\n",
    "\n",
    "# Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üìä ÏµúÏ¢Ö Í≤∞Í≥º\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Fold: {fold_result['fold']}\")\n",
    "print(f\"  ÏµúÏ†Ä Val Loss: {fold_result['best_val_loss']:.4f}\")\n",
    "print(f\"  Ìï¥Îãπ Val Acc: {fold_result['best_val_acc']:.2f}%\")\n",
    "print(f\"  ÌïôÏäµ ÏôÑÎ£å ÏóêÌè≠: {fold_result['stopped_epoch']}\")\n",
    "print(f\"  Freeze Î™®Îìú: {fold_result['freeze_mode']}\")\n",
    "print(f\"  ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞: {fold_result['trainable_params']:,} / {fold_result['total_params']:,}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4580a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ÌïôÏäµ Í≥°ÏÑ† ÏãúÍ∞ÅÌôî (4Í∞ú Í∑∏ÎûòÌîÑ) ==========\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for result in all_fold_results:\n",
    "    fold_idx = result['fold']\n",
    "    history = result['history']\n",
    "    stopped_epoch = result['stopped_epoch']\n",
    "    best_val_loss = result['best_val_loss']\n",
    "    \n",
    "    # 1. Loss Í∑∏ÎûòÌîÑ\n",
    "    ax1 = axes[0, 0]\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax1.plot(epochs, history['train_loss'], label='Train Loss', \n",
    "             marker='o', linewidth=2, alpha=0.8, color='#1f77b4')\n",
    "    ax1.plot(epochs, history['val_loss'], label='Val Loss', \n",
    "             marker='s', linewidth=2, alpha=0.8, color='#ff7f0e')\n",
    "    \n",
    "    # ÏµúÏ†Ä Val Loss ÏßÄÏ†ê ÌëúÏãú\n",
    "    best_epoch = np.argmin(history['val_loss']) + 1\n",
    "    ax1.scatter(best_epoch, best_val_loss, color='red', s=250, zorder=5, \n",
    "                marker='*', edgecolors='black', linewidths=2,\n",
    "                label=f'Best (Epoch {best_epoch})')\n",
    "    \n",
    "    # Early Stopping ÏßÄÏ†ê ÌëúÏãú\n",
    "    if stopped_epoch < num_epochs:\n",
    "        ax1.axvline(stopped_epoch, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.5, label=f'Early Stop (E{stopped_epoch})')\n",
    "    \n",
    "    ax1.set_title(f'Fold {fold_idx} - Loss (Multiclass Log Loss)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.legend(loc='best', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Accuracy Í∑∏ÎûòÌîÑ\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(epochs, history['train_acc'], label='Train Acc', \n",
    "             marker='o', linewidth=2, alpha=0.8, color='#2ca02c')\n",
    "    ax2.plot(epochs, history['val_acc'], label='Val Acc', \n",
    "             marker='s', linewidth=2, alpha=0.8, color='#d62728')\n",
    "    \n",
    "    # ÏµúÍ≥† Val Acc ÏßÄÏ†ê ÌëúÏãú\n",
    "    best_acc_epoch = np.argmax(history['val_acc']) + 1\n",
    "    best_val_acc = max(history['val_acc'])\n",
    "    ax2.scatter(best_acc_epoch, best_val_acc, color='green', s=250, zorder=5,\n",
    "                marker='*', edgecolors='black', linewidths=2,\n",
    "                label=f'Best (Epoch {best_acc_epoch})')\n",
    "    \n",
    "    if stopped_epoch < num_epochs:\n",
    "        ax2.axvline(stopped_epoch, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.5, label=f'Early Stop (E{stopped_epoch})')\n",
    "    \n",
    "    ax2.set_title(f'Fold {fold_idx} - Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.legend(loc='best', fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Learning Rate Í∑∏ÎûòÌîÑ\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(epochs, history['learning_rates'], marker='o', linewidth=2, \n",
    "             color='purple', alpha=0.8, label='Learning Rate')\n",
    "    ax3.set_title(f'Fold {fold_idx} - Learning Rate Schedule', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch', fontsize=12)\n",
    "    ax3.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.legend(loc='best', fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3, which='both')\n",
    "    \n",
    "    if stopped_epoch < num_epochs:\n",
    "        ax3.axvline(stopped_epoch, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.5)\n",
    "    \n",
    "    # 4. Train vs Val ÎπÑÍµê (Loss & Acc)\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Loss Ï∞®Ïù¥\n",
    "    loss_diff = np.array(history['train_loss']) - np.array(history['val_loss'])\n",
    "    ax4_twin = ax4.twinx()\n",
    "    \n",
    "    ax4.plot(epochs, loss_diff, marker='o', linewidth=2, \n",
    "            color='#e377c2', alpha=0.7, label='Loss Diff (Train - Val)')\n",
    "    ax4.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    ax4.set_xlabel('Epoch', fontsize=12)\n",
    "    ax4.set_ylabel('Loss Difference', fontsize=12, color='#e377c2')\n",
    "    ax4.tick_params(axis='y', labelcolor='#e377c2')\n",
    "    \n",
    "    # Accuracy Ï∞®Ïù¥\n",
    "    acc_diff = np.array(history['train_acc']) - np.array(history['val_acc'])\n",
    "    ax4_twin.plot(epochs, acc_diff, marker='s', linewidth=2,\n",
    "                 color='#bcbd22', alpha=0.7, label='Acc Diff (Train - Val)')\n",
    "    ax4_twin.set_ylabel('Accuracy Difference (%)', fontsize=12, color='#bcbd22')\n",
    "    ax4_twin.tick_params(axis='y', labelcolor='#bcbd22')\n",
    "    \n",
    "    ax4.set_title(f'Fold {fold_idx} - Overfitting Î™®ÎãàÌÑ∞ÎßÅ', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Legend ÌÜµÌï©\n",
    "    lines1, labels1 = ax4.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax4_twin.get_legend_handles_labels()\n",
    "    ax4.legend(lines1 + lines2, labels1 + labels2, loc='best', fontsize=9)\n",
    "    \n",
    "    if stopped_epoch < num_epochs:\n",
    "        ax4.axvline(stopped_epoch, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs(f'./plots/{name}', exist_ok=True)\n",
    "plt.savefig(f'./plots/{name}/{name}_losscurve_detailed.png', \n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ========== ÌÜµÍ≥Ñ Ï∂úÎ†• ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"üìà {name} ÌïôÏäµ ÌÜµÍ≥Ñ ÏÉÅÏÑ∏\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Ï¥àÍ∏∞ Train Loss: {history['train_loss'][0]:.4f}\")\n",
    "print(f\"ÏµúÏ¢Ö Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"ÏµúÏ†Ä Train Loss: {min(history['train_loss']):.4f} (Epoch {np.argmin(history['train_loss'])+1})\")\n",
    "print(f\"\\nÏ¥àÍ∏∞ Val Loss: {history['val_loss'][0]:.4f}\")\n",
    "print(f\"ÏµúÏ¢Ö Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"ÏµúÏ†Ä Val Loss: {min(history['val_loss']):.4f} (Epoch {np.argmin(history['val_loss'])+1})\")\n",
    "print(f\"\\nÏ¥àÍ∏∞ Train Acc: {history['train_acc'][0]:.2f}%\")\n",
    "print(f\"ÏµúÏ¢Ö Train Acc: {history['train_acc'][-1]:.2f}%\")\n",
    "print(f\"ÏµúÍ≥† Train Acc: {max(history['train_acc']):.2f}% (Epoch {np.argmax(history['train_acc'])+1})\")\n",
    "print(f\"\\nÏ¥àÍ∏∞ Val Acc: {history['val_acc'][0]:.2f}%\")\n",
    "print(f\"ÏµúÏ¢Ö Val Acc: {history['val_acc'][-1]:.2f}%\")\n",
    "print(f\"ÏµúÍ≥† Val Acc: {max(history['val_acc']):.2f}% (Epoch {np.argmax(history['val_acc'])+1})\")\n",
    "print(f\"\\nÏ¥àÍ∏∞ LR: {history['learning_rates'][0]:.6f}\")\n",
    "print(f\"ÏµúÏ¢Ö LR: {history['learning_rates'][-1]:.6f}\")\n",
    "print(f\"LR Î≥ÄÍ≤Ω ÌöüÏàò: {len(set(history['learning_rates'])) - 1}Ìöå\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========== Overfitting Î∂ÑÏÑù ==========\n",
    "final_loss_gap = history['train_loss'][-1] - history['val_loss'][-1]\n",
    "final_acc_gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç Overfitting Î∂ÑÏÑù\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ÏµúÏ¢Ö Loss Ï∞®Ïù¥ (Train - Val): {final_loss_gap:+.4f}\")\n",
    "print(f\"ÏµúÏ¢Ö Acc Ï∞®Ïù¥ (Train - Val): {final_acc_gap:+.2f}%\")\n",
    "\n",
    "if final_acc_gap > 10:\n",
    "    print(\"‚ö†Ô∏è Í≤ΩÍ≥†: Ïã¨Í∞ÅÌïú Overfitting Í∞êÏßÄ! (Acc Ï∞®Ïù¥ > 10%)\")\n",
    "elif final_acc_gap > 5:\n",
    "    print(\"‚ö†Ô∏è Ï£ºÏùò: ÏïΩÍ∞ÑÏùò Overfitting Í∞êÏßÄ (Acc Ï∞®Ïù¥ > 5%)\")\n",
    "else:\n",
    "    print(\"‚úÖ ÏñëÌò∏: OverfittingÏù¥ Ïûò Ï†úÏñ¥ÎêòÍ≥† ÏûàÏäµÎãàÎã§.\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80840b6",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ab894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Submission ÌååÏùº ÏÉùÏÑ± (ÌïôÏäµÎêú Î™®Îç∏ ÏÇ¨Ïö©) ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîÆ ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏° ÏãúÏûë\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\n",
    "test_dataset = DriverDataset(\n",
    "    clahe_test_dir, driver_df, [], \n",
    "    transform=transform_eval, is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"ÌÖåÏä§Ìä∏ ÏÉòÌîå: {len(test_dataset)}Í∞ú\")\n",
    "\n",
    "# 2. Î™®Îç∏ ÏÉùÏÑ±\n",
    "model = ResNetDWithHead(\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "# 3. Ï†ÄÏû•Îêú Í∞ÄÏ§ëÏπò Î°úÎìú\n",
    "best_model_path = f'models/{name}.pth'\n",
    "print(f\"\\nüìÅ Î™®Îç∏ Î°úÎìú Ï§ë: {best_model_path}...\")\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Î°úÎìúÎêú Î™®Îç∏ Ï†ïÎ≥¥ Ï∂úÎ†•\n",
    "    print(f\"‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å!\")\n",
    "    print(f\"   - Fold: {checkpoint.get('fold', 'N/A')}\")\n",
    "    print(f\"   - Epoch: {checkpoint.get('epoch', 'N/A')+1}\")\n",
    "    print(f\"   - Val Loss: {checkpoint.get('val_loss', 'N/A'):.4f}\")\n",
    "    print(f\"   - Val logLoss: {checkpoint.get('val_logloss', 'N/A'):.4f}\")\n",
    "    print(f\"   - Val Acc: {checkpoint.get('val_acc', 'N/A'):.2f}%\")\n",
    "    print(f\"   - Val Macro-F1: {checkpoint.get('val_macro_f1', 'N/A'):.4f}\")\n",
    "    print(f\"   - Val LogLoss: {checkpoint.get('val_logloss', 'N/A'):.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"üö® Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}\")\n",
    "    raise\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 4. ÏòàÏ∏° Ïã§Ìñâ\n",
    "predictions = []\n",
    "img_names = []\n",
    "\n",
    "print(\"\\nüîÆ ÏòàÏ∏° ÏßÑÌñâ Ï§ë...\")\n",
    "with torch.no_grad():\n",
    "    for images, filenames in tqdm(test_loader, desc='ÏòàÏ∏°'):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # 5. ÌôïÎ•† (Softmax) Í≥ÑÏÇ∞\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        predictions.append(probs.cpu().numpy())\n",
    "        img_names.extend(filenames)\n",
    "\n",
    "final_predictions = np.vstack(predictions)\n",
    "print(f\"\\n‚úÖ ÏòàÏ∏° ÏôÑÎ£å: {final_predictions.shape}\")\n",
    "\n",
    "# 6. Submission ÌååÏùº ÏÉùÏÑ±\n",
    "class_cols = [f'c{i}' for i in range(num_classes)]\n",
    "\n",
    "submission_data = {'img': img_names}\n",
    "for i, col in enumerate(class_cols):\n",
    "    submission_data[col] = final_predictions[:, i]\n",
    "\n",
    "submission = pd.DataFrame(submission_data)\n",
    "\n",
    "# 7. ÌååÏùº Ï†ÄÏû•\n",
    "os.makedirs('./submissions', exist_ok=True)\n",
    "submission.to_csv(f\"./submissions/{name}_submission.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"‚úÖ Submission ÌååÏùº ÏÉùÏÑ± ÏôÑÎ£å!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÅ Ï†ÄÏû• Í≤ΩÎ°ú: ./submissions/{name}_submission.csv\")\n",
    "print(f\"üìä Ï¥ù ÏòàÏ∏° Ïù¥ÎØ∏ÏßÄ: {len(submission)}Í∞ú\")\n",
    "print(f\"üìã ÌÅ¥ÎûòÏä§Î≥Ñ ÌèâÍ∑† ÌôïÎ•†:\")\n",
    "for col in class_cols:\n",
    "    print(f\"   {col}: {submission[col].mean():.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìã Submission ÏÉòÌîå (Ï≤òÏùå 5Í∞ú):\")\n",
    "print(submission.head())\n",
    "\n",
    "print(\"\\nüìã Submission ÏÉòÌîå (ÎßàÏßÄÎßâ 5Í∞ú):\")\n",
    "print(submission.tail())\n",
    "\n",
    "# 8. ÌôïÎ•† Î∂ÑÌè¨ Í≤ÄÏ¶ù (Ìï©Ïù¥ 1Ïù∏ÏßÄ ÌôïÏù∏)\n",
    "prob_sums = submission[class_cols].sum(axis=1)\n",
    "print(f\"\\n‚úÖ ÌôïÎ•† Ìï© Í≤ÄÏ¶ù: min={prob_sums.min():.6f}, max={prob_sums.max():.6f}, mean={prob_sums.mean():.6f}\")\n",
    "if not np.allclose(prob_sums, 1.0, atol=1e-5):\n",
    "    print(\"‚ö†Ô∏è Í≤ΩÍ≥†: ÏùºÎ∂Ä ÏÉòÌîåÏùò ÌôïÎ•† Ìï©Ïù¥ 1Ïù¥ ÏïÑÎãôÎãàÎã§!\")\n",
    "else:\n",
    "    print(\"‚úÖ Î™®Îì† ÏÉòÌîåÏùò ÌôïÎ•† Ìï©Ïù¥ 1ÏûÖÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c18454",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏÖÄ Ï∂îÍ∞Ä: Confusion Matrix Î∂ÑÏÑù\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_predictions(model, val_loader, device):\n",
    "    \"\"\"Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Î°ú ÏÉÅÏÑ∏ Î∂ÑÏÑù\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc='ÏòàÏ∏° Ï§ë'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images, labels)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    all_probs = np.vstack(all_probs)\n",
    "    return np.array(all_labels), np.array(all_preds), all_probs\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "model = ResNetDWithHead(\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# ÏòàÏ∏°\n",
    "fold_info = fold_splits[1] \n",
    "\n",
    "fold_idx = fold_info['fold']\n",
    "train_drivers = fold_info['train_drivers']\n",
    "val_drivers = fold_info['val_drivers']\n",
    "\n",
    "val_dataset = DriverDataset(\n",
    "    train_dir, driver_df, val_drivers,\n",
    "    transform=transform_eval, is_test=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "y_true, y_pred, y_probs = analyze_predictions(model, val_loader, device)\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "class_names = [f'c{i}' for i in range(10)]\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./plots/{name}/confusion_matrix_{name}.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 2. Normalized Confusion Matrix (ÎπÑÏú®)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='YlOrRd',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Normalized Confusion Matrix (%)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "os.makedirs(f'plots/{name}', exist_ok=True)\n",
    "plt.savefig(f'plots/{name}/confusion_matrix_normalized_{name}.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 3. Classification Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä Classification Report\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌÅ¥ÎûòÏä§Î≥Ñ ÏÑ±Îä• Î∂ÑÏÑù\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "if 'y_true' not in globals() or 'y_pred' not in globals() or 'y_probs' not in globals():\n",
    "    print(\"y_true/y_pred/y_probsÍ∞Ä ÏóÜÏùå ‚Äî analyze_predictions Ïã§Ìñâ Ï§ë...\")\n",
    "    y_true, y_pred, y_probs = analyze_predictions(model, val_loader, device)\n",
    "else:\n",
    "    print(\"y_true/y_pred/y_probs Ïù¥ÎØ∏ Ï°¥Ïû¨, Ïû¨Í≥ÑÏÇ∞ ÏÉùÎûµ\")\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=None\n",
    ")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏúºÎ°ú Ï†ïÎ¶¨\n",
    "class_performance = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support,\n",
    "    'Accuracy': [accuracy_score(y_true[y_true==i], y_pred[y_true==i]) \n",
    "                 if np.sum(y_true==i) > 0 else 0 for i in range(10)]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà ÌÅ¥ÎûòÏä§Î≥Ñ ÏÑ±Îä•\")\n",
    "print(\"=\"*70)\n",
    "print(class_performance.to_string(index=False))\n",
    "\n",
    "# ÏãúÍ∞ÅÌôî\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Precision\n",
    "axes[0, 0].bar(class_names, precision, color='skyblue', alpha=0.8)\n",
    "axes[0, 0].set_title('Precision by Class', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Precision')\n",
    "axes[0, 0].set_ylim([0, 1.1])\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[0, 1].bar(class_names, recall, color='lightcoral', alpha=0.8)\n",
    "axes[0, 1].set_title('Recall by Class', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Recall')\n",
    "axes[0, 1].set_ylim([0, 1.1])\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# F1-Score\n",
    "axes[1, 0].bar(class_names, f1, color='lightgreen', alpha=0.8)\n",
    "axes[1, 0].set_title('F1-Score by Class', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('F1-Score')\n",
    "axes[1, 0].set_ylim([0, 1.1])\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Support\n",
    "axes[1, 1].bar(class_names, support, color='plum', alpha=0.8)\n",
    "axes[1, 1].set_title('Support (Sample Count) by Class', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs(f'plots/{name}', exist_ok=True)\n",
    "plt.savefig(f'plots/{name}/class_performance_{name}.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24862329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Î∂ÑÏÑù\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# One-hot encoding\n",
    "y_true_bin = label_binarize(y_true, classes=range(10))\n",
    "\n",
    "# Í∞Å ÌÅ¥ÎûòÏä§Î≥Ñ ROC Curve\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{class_names[i]} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves (Multi-class)', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/{name}/roc_curves_{name}.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71559815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM Íµ¨ÌòÑ\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"Grad-CAM (ResNet Ìò∏Ìôò)\"\"\"\n",
    "    def __init__(self, model, target_module=None):\n",
    "        self.model = model\n",
    "        self.target_module = target_module\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        # target_moduleÏù¥ NoneÏù¥Î©¥ Î™®Îç∏ÏóêÏÑú ÎßàÏßÄÎßâ Conv2dÎ•º Ï∞æÏïÑ ÏÇ¨Ïö©\n",
    "        if self.target_module is None:\n",
    "            self.target_module, _ = get_last_conv_layer_resnet(self.model)\n",
    "            if self.target_module is None:\n",
    "                raise RuntimeError(\"ÎßàÏßÄÎßâ Conv Î†àÏù¥Ïñ¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "        # ÌõÖ Îì±Î°ù (forward / backward)\n",
    "        self.target_module.register_forward_hook(self._save_activation)\n",
    "        # ÏµúÏã† PyTorchÏóêÏÑúÎäî register_full_backward_hook Í∂åÏû•, ÏóÜÏúºÎ©¥ register_backward_hook ÏÇ¨Ïö©\n",
    "        if hasattr(self.target_module, \"register_full_backward_hook\"):\n",
    "            self.target_module.register_full_backward_hook(self._save_gradient)\n",
    "        else:\n",
    "            # Íµ¨Î≤ÑÏ†Ñ Ìò∏Ìôò\n",
    "            self.target_module.register_backward_hook(self._save_gradient)\n",
    "\n",
    "    def _save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def _save_gradient(self, module, grad_input, grad_output):\n",
    "        # grad_outputÎäî tuple\n",
    "        self.gradients = grad_output[0].detach()\n",
    "\n",
    "    def generate_cam(self, input_tensor, target_class=None):\n",
    "        \"\"\"\n",
    "        input_tensor: [1, C, H, W] (Ïù¥ÎØ∏ Ï†ïÍ∑úÌôîÎêú ÌÖêÏÑú)`\n",
    "        target_class: int or None (NoneÏù¥Î©¥ argmax ÏÇ¨Ïö©)\n",
    "        returns: cam (H, W) float numpy [0..1], logits tensor\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        logits = self.model(input_tensor)  # ÏùºÎ∞ò forward\n",
    "\n",
    "        if target_class is None:\n",
    "            target_class = int(logits.argmax(dim=1)[0])\n",
    "\n",
    "        # backward on the score of target_class\n",
    "        self.model.zero_grad()\n",
    "        score = logits[0, target_class]\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        if self.gradients is None or self.activations is None:\n",
    "            raise RuntimeError(\"Gradients or activations not recorded. Check hooks.\")\n",
    "\n",
    "        # gradients: [1, C, H, W] -> use [C, H, W]\n",
    "        gradients = self.gradients[0].cpu()\n",
    "        activations = self.activations[0].cpu()\n",
    "\n",
    "        # global average pooling of gradients -> weights [C]\n",
    "        weights = gradients.mean(dim=(1, 2))  # [C]\n",
    "\n",
    "        # weighted sum of activations\n",
    "        cam = (weights[:, None, None] * activations).sum(dim=0)  # [H, W]\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam - cam.min()\n",
    "        if cam.max() > 0:\n",
    "            cam = cam / cam.max()\n",
    "        cam_np = cam.numpy().astype(np.float32)\n",
    "\n",
    "        return cam_np, logits\n",
    "\n",
    "def get_last_conv_layer_resnet(model):\n",
    "    \"\"\"\n",
    "    ResNet Í≥ÑÏó¥ Î™®Îç∏ÏóêÏÑú ÎßàÏßÄÎßâ nn.Conv2d Î™®ÎìàÏùÑ Ï∞æÏïÑ Î∞òÌôò\n",
    "    Î∞òÌôò: (module, name) ÎòêÎäî (None, None)\n",
    "    \"\"\"\n",
    "    for name, module in reversed(list(model.named_modules())):\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            return module, name\n",
    "    return None, None\n",
    "\n",
    "def visualize_gradcam(model, image, true_label, pred_label, device):\n",
    "    \"\"\"Grad-CAM ÏãúÍ∞ÅÌôî (ResNet)\"\"\"\n",
    "    # target layer Ï∞æÍ∏∞/ÏÑ§Ï†ï\n",
    "    target_module, target_name = get_last_conv_layer_resnet(model)\n",
    "    if target_module is None:\n",
    "        print(\"‚ö†Ô∏è ÎßàÏßÄÎßâ Conv Î†àÏù¥Ïñ¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "        return\n",
    "\n",
    "    gradcam = GradCAM(model, target_module)\n",
    "\n",
    "    # Ïù¥ÎØ∏ÏßÄÎäî [C, H, W] (Ï†ïÍ∑úÌôîÎêú ÌÖêÏÑú)\n",
    "    input_tensor = image.unsqueeze(0).to(device)\n",
    "    cam, output = gradcam.generate_cam(input_tensor, target_class=pred_label)\n",
    "\n",
    "    # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Î≥µÏõê (Ï†ïÍ∑úÌôî Ïó≠Î≥ÄÌôò)\n",
    "    img_np = image.cpu().numpy().transpose(1, 2, 0)\n",
    "    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    # camÏùÄ [H, W], float [0..1] -> uint8Î°ú Î≥ÄÌôò ÌõÑ PIL Î¶¨ÏÇ¨Ïù¥Ï¶à\n",
    "    H, W = img_np.shape[:2]\n",
    "    cam_uint8 = np.uint8(255 * cam)\n",
    "    cam_pil = Image.fromarray(cam_uint8).resize((W, H), Image.BILINEAR)\n",
    "    cam_resized = np.array(cam_pil) / 255.0\n",
    "\n",
    "    # ÏãúÍ∞ÅÌôî\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    axes[0].imshow(img_np)\n",
    "    axes[0].set_title(f'Original\\nTrue: c{true_label}', fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(img_np)\n",
    "    axes[1].imshow(cam_resized, cmap='jet', alpha=0.5)\n",
    "    axes[1].set_title(f'Grad-CAM\\nPred: c{pred_label}', fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(cam_resized, cmap='jet')\n",
    "    axes[2].set_title('Heatmap', fontsize=12)\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ÏÉòÌîå Ïù¥ÎØ∏ÏßÄÏóê Grad-CAM Ï†ÅÏö©\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî• Grad-CAM ÏãúÍ∞ÅÌôî\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú (ResNetDWithHead ÎûòÌçº ÏÇ¨Ïö©)\n",
    "model = ResNetDWithHead(\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏÉòÌîå Ï∂îÏ∂ú (Ï†ïÎãµ/Ïò§Îãµ Í∞ÅÍ∞Å)\n",
    "correct_samples = []\n",
    "incorrect_samples = []\n",
    "\n",
    "for i, (images, labels) in enumerate(tqdm(val_loader)):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    for j in range(len(images)):\n",
    "        if preds[j] == labels[j]:\n",
    "            correct_samples.append((images[j], labels[j].item(), preds[j].item()))\n",
    "        elif preds[j] != labels[j]:\n",
    "            incorrect_samples.append((images[j], labels[j].item(), preds[j].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ï†ïÎãµ ÏòàÏ∏° ÏÉòÌîå (Í∞Å ÌÅ¥ÎûòÏä§ Î≥Ñ 5Í∞ú)\")\n",
    "corr_class_samples = {i: [] for i in range(10)}\n",
    "max_samples = 5\n",
    "os.makedirs(f'plots/{name}/gradcam_correct', exist_ok=True)\n",
    "\n",
    "for img, true_label, pred_label in correct_samples:\n",
    "\tcurr = corr_class_samples[true_label]\n",
    "\tif len(curr) < max_samples:\n",
    "\t\tcurr.append((img, true_label, pred_label))\n",
    "\n",
    "for class_idx in range(10):\n",
    "\tif corr_class_samples[class_idx]:\n",
    "\t\tprint(f\"{class_idx} grad cam\")\n",
    "\t\tfor idx, (img, true_label, pred_label) in enumerate(corr_class_samples[class_idx]):\n",
    "\t\t\tfig = visualize_gradcam(model, img, true_label, pred_label, device)\n",
    "\t\t\tplt.savefig(f'plots/{name}/gradcam_correct/gradcam_correct_c{class_idx}_{idx}_{name}.png', dpi=300, bbox_inches='tight')\n",
    "\t\t\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ï†ïÎãµ ÏòàÏ∏° ÏÉòÌîå (Í∞Å ÌÅ¥ÎûòÏä§ Î≥Ñ 5Í∞ú)\")\n",
    "incorr_class_samples = {i: [] for i in range(10)}\n",
    "max_samples = 5\n",
    "os.makedirs(f'plots/{name}/gradcam_incorrect', exist_ok=True)\n",
    "\n",
    "for img, true_label, pred_label in incorrect_samples:\n",
    "\tcurr = incorr_class_samples[true_label]\n",
    "\tif len(curr) < max_samples:\n",
    "\t\tcurr.append((img, true_label, pred_label))\n",
    "\n",
    "for class_idx in range(10):\n",
    "\tif incorr_class_samples[class_idx]:\n",
    "\t\tprint(f\"{class_idx} grad cam\")\n",
    "\t\tfor idx, (img, true_label, pred_label) in enumerate(incorr_class_samples[class_idx]):\n",
    "\t\t\tfig = visualize_gradcam(model, img, true_label, pred_label, device)\n",
    "\t\t\tplt.savefig(f'plots/{name}/gradcam_incorrect/gradcam_incorrect_c{class_idx}_{idx}_{name}.png', dpi=300, bbox_inches='tight')\n",
    "\t\t\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb7c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'resnet_{version}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Map Ï∂îÏ∂ú Î∞è ÏãúÍ∞ÅÌôî\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Ï§ëÍ∞Ñ Î†àÏù¥Ïñ¥Ïùò Feature Map Ï∂îÏ∂ú\"\"\"\n",
    "    \n",
    "    def __init__(self, model, layer_names):\n",
    "        self.model = model\n",
    "        self.layer_names = layer_names\n",
    "        self.features = {}\n",
    "        \n",
    "        # Hook Îì±Î°ù\n",
    "        for name, layer in model.named_modules():\n",
    "            if name in layer_names:\n",
    "                layer.register_forward_hook(self.save_feature(name))\n",
    "    \n",
    "    def save_feature(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.features[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    def extract(self, x):\n",
    "        self.features = {}\n",
    "        _ = self.model(x)\n",
    "        return self.features\n",
    "\n",
    "def visualize_feature_maps(features, layer_name, max_channels=16):\n",
    "    \"\"\"Feature Map ÏãúÍ∞ÅÌôî\"\"\"\n",
    "    feature = features[layer_name][0]  # Ï≤´ Î≤àÏß∏ Î∞∞Ïπò\n",
    "    num_channels = min(feature.shape[0], max_channels)\n",
    "    \n",
    "    # Grid ÌÅ¨Í∏∞ Í≥ÑÏÇ∞\n",
    "    grid_size = int(np.ceil(np.sqrt(num_channels)))\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_channels):\n",
    "        fmap = feature[i].cpu().numpy()\n",
    "        axes[i].imshow(fmap, cmap='viridis')\n",
    "        axes[i].set_title(f'Ch {i}', fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Îπà subplot Ïà®Í∏∞Í∏∞\n",
    "    for i in range(num_channels, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Feature Maps: {layer_name}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Inception V4Ïùò Ï£ºÏöî Î†àÏù¥Ïñ¥ Ïù¥Î¶Ñ ÌôïÏù∏\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç Î™®Îç∏ Íµ¨Ï°∞ ÌÉêÏÉâ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for n, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        print(f\"Conv Layer: {n}\")\n",
    "\n",
    "# Ï£ºÏöî Î†àÏù¥Ïñ¥ ÏÑ†ÌÉù (ÏòàÏãú)\n",
    "target_layers = [\n",
    "    'backbone.conv1.6',\n",
    "    'backbone.layer1.0.conv3',\n",
    "    'backbone.layer2.0.conv3',\n",
    "    'backbone.layer4.0.conv3',\n",
    "    'backbone.layer4.2.conv3'\n",
    "\n",
    "]\n",
    "\n",
    "# ÏÉòÌîå Ïù¥ÎØ∏ÏßÄÎ°ú Feature Map Ï∂îÏ∂ú\n",
    "sample_image, sample_label = next(iter(val_loader))\n",
    "sample_image = sample_image[0:1].to(device)  # Ï≤´ Î≤àÏß∏ Ïù¥ÎØ∏ÏßÄ\n",
    "\n",
    "extractor = FeatureExtractor(model, target_layers)\n",
    "features = extractor.extract(sample_image)\n",
    "\n",
    "# Í∞Å Î†àÏù¥Ïñ¥ ÏãúÍ∞ÅÌôî\n",
    "for layer_name in target_layers:\n",
    "    if layer_name in features:\n",
    "        fig = visualize_feature_maps(features, layer_name, max_channels=16)\n",
    "        plt.savefig(f'plots/{name}/feature_map_{layer_name.replace(\".\", \"_\")}.png', dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE/UMAPÏúºÎ°ú ÏûÑÎ≤†Îî© ÏãúÍ∞ÅÌôî\n",
    "from sklearn.manifold import TSNE\n",
    "# pip install umap-learn\n",
    "# from umap import UMAP\n",
    "\n",
    "def extract_embeddings(model, dataloader, device):\n",
    "    \"\"\"ÎßàÏßÄÎßâ FC Î†àÏù¥Ïñ¥ Ïù¥Ï†ÑÏùò ÏûÑÎ≤†Îî© Ï∂îÏ∂ú\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    # FC Î†àÏù¥Ïñ¥ Ïù¥Ï†Ñ Ï∂úÎ†• Ï∂îÏ∂úÏùÑ ÏúÑÌïú Hook\n",
    "    features = []\n",
    "    def hook(module, input, output):\n",
    "        features.append(input[0].detach())\n",
    "    \n",
    "\n",
    "    handle = model.classifier.register_forward_hook(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, lbls in tqdm(dataloader, desc='ÏûÑÎ≤†Îî© Ï∂îÏ∂ú'):\n",
    "            images = images.to(device)\n",
    "            _ = model(images)\n",
    "            \n",
    "            embeddings.append(features[-1].cpu().numpy())\n",
    "            labels.extend(lbls.numpy())\n",
    "            features.clear()\n",
    "    \n",
    "    handle.remove()\n",
    "    \n",
    "    embeddings = np.vstack(embeddings)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return embeddings, labels\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Ï∂îÏ∂ú\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß¨ ÏûÑÎ≤†Îî© Ï∂îÏ∂ú Ï§ë...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "embeddings, labels = extract_embeddings(model, val_loader, device)\n",
    "print(f\"ÏûÑÎ≤†Îî© shape: {embeddings.shape}\")\n",
    "\n",
    "# t-SNE ÏãúÍ∞ÅÌôî\n",
    "print(\"\\nüìä t-SNE Í≥ÑÏÇ∞ Ï§ë...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0], \n",
    "    embeddings_2d[:, 1], \n",
    "    c=labels, \n",
    "    cmap='tab10', \n",
    "    s=10, \n",
    "    alpha=0.6\n",
    ")\n",
    "plt.colorbar(scatter, label='Class', ticks=range(10))\n",
    "plt.title('t-SNE Visualization of Learned Embeddings', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/{name}/tsne_embeddings.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ÌÅ¥ÎûòÏä§Î≥ÑÎ°ú ÏÉâÏÉÅ Íµ¨Î∂ÑÌïòÏó¨ Legend Ï∂îÍ∞Ä\n",
    "plt.figure(figsize=(16, 12))\n",
    "for class_idx in range(10):\n",
    "    mask = labels == class_idx\n",
    "    plt.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        label=f'c{class_idx}',\n",
    "        s=20,\n",
    "        alpha=0.7\n",
    "    )\n",
    "plt.title('t-SNE Visualization by Class', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/{name}/tsne_embeddings_by_class.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563b0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5aede7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
