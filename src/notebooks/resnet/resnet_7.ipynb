{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bee68b1",
   "metadata": {},
   "source": [
    "# Resnet_7\n",
    "ì„œí˜„ì´ v0.01ì—ì„œ ëª¨ë¸ë§Œ ë‚´ê±¸ë¡œ ë°”ê¾¼ê±°  \n",
    "ê·¼ë° í•™ìŠµí•œ í´ë“œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤ê³  í•¨.\n",
    "ê¸°ì¡´ ì½”ë“œì—ì„œ weight decay 0.05(ì•„ë‹ˆ ì´ê±° ì™¤ì¼€ ì»¸ì—ˆì§€..) -> 1e-4,  \n",
    "cosine annealing eta_min = 1e-6 ì¶”ê°€í•¨  \n",
    "ë˜ img size 224ë¡œ ì¤„ì´ê³  batch sizeë„ 32ë¡œ ì¤„ì„\n",
    "label smoothingë„ 0.1ì—ì„œ 0.05ë¡œ ì¤„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f06041a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d040029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0089e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ============================================================\n",
    "# ì¬í˜„ì„± ë³´ì¥ (Reproducibility)\n",
    "# ============================================================\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"ëª¨ë“  ë‚œìˆ˜ ìƒì„±ê¸°ì˜ ì‹œë“œë¥¼ ê³ ì •í•˜ì—¬ ì¬í˜„ì„± ë³´ì¥\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # multi-GPU\n",
    "    \n",
    "    # CUDA ê²°ì •ë¡ ì  ë™ì‘ í™œì„±í™”\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # PyTorch ê²°ì •ë¡ ì  ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© (PyTorch 1.8+)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    \n",
    "    print(f\"âœ… Random Seed ê³ ì • ì™„ë£Œ: {seed}\")\n",
    "    print(f\"   - Python random: {seed}\")\n",
    "    print(f\"   - NumPy: {seed}\")\n",
    "    print(f\"   - PyTorch: {seed}\")\n",
    "    print(f\"   - CUDA: {seed}\")\n",
    "    print(f\"   - cuDNN Deterministic: True\")\n",
    "    print(f\"   - cuDNN Benchmark: False\")\n",
    "    print(f\"   - PyTorch Deterministic Algorithms: True\")\n",
    "\n",
    "# Seed ì ìš©\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import multiprocessing\n",
    "\n",
    "# # Windows í™˜ê²½ì—ì„œ num_workers > 0 ì‚¬ìš©ì„ ìœ„í•œ í•„ìˆ˜ ì„¤ì •\n",
    "# try:\n",
    "#     # 'spawn' ë°©ì‹ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì•ˆì „í•œ í”„ë¡œì„¸ìŠ¤ ìƒì„±ì„ ìœ ë„\n",
    "#     torch.multiprocessing.set_start_method('spawn', force=True) \n",
    "#     print(\"PyTorch multiprocessing start method set to 'spawn'.\")\n",
    "# except RuntimeError as e:\n",
    "#     # ì´ë¯¸ ì„¤ì •ëœ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬\n",
    "#     print(f\"Start method already set or error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02842c98",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c6361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "num_epochs = 50\n",
    "# learning_rate = 0.0003 -> train section\n",
    "num_workers = 0\n",
    "version = \"7\"\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ\n",
    "base_dir = r'c:\\Users\\USER\\PycharmProjects\\DeepLearning-Term-Proj'\n",
    "driver_csv_path = os.path.join(base_dir, 'data', 'driver_imgs_list.csv')\n",
    "train_dir = os.path.join(base_dir, 'data', 'imgs', 'train')\n",
    "test_dir = os.path.join(base_dir, 'data', 'imgs', 'test')\n",
    "name = f'resnet_{version}'\n",
    "\n",
    "print(f\"Train directory: {train_dir}\")\n",
    "print(f\"Test directory: {test_dir}\")\n",
    "print(f\"ì´ë¯¸ì§€ í¬ê¸°: {img_size}x{img_size}\")\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_df = pd.read_csv(driver_csv_path)\n",
    "\n",
    "print(f\"ê³ ìœ  ìš´ì „ì ìˆ˜: {driver_df['subject'].nunique()}ëª…\")\n",
    "print(f\"ìš´ì „ì ëª©ë¡: {sorted(driver_df['subject'].unique())}\")\n",
    "\n",
    "# driver_counts = driver_df['subject'].value_counts().sort_index()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.bar(driver_counts.index.astype(str), driver_counts.values, color='C0', alpha=0.9)\n",
    "# plt.xlabel('Driver')\n",
    "# plt.ylabel('Image Count')\n",
    "# plt.title('Images per Driver')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.grid(axis='y', alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ba3cbe",
   "metadata": {},
   "source": [
    "# 5-Fold Cross Validation\n",
    "foldë³„ ìš´ì „ì ëª©ë¡ì„ ë¯¸ë¦¬ ë‚˜ëˆ„ê¸´ í•˜ë˜ ì´ˆê¸° ì‹¤í—˜ ë‹¨ê³„ì—ì„œëŠ” í•œ í´ë“œ(fold 2, í•™ìŠµ ë°ì´í„°ê°€ ê°€ì¥ ë§ì•„ì„œ..)ë§Œ ì‚¬ìš©í•˜ê³  ì´í›„ ë§ˆë¬´ë¦¬ ë‹¨ê³„ì—ì„œ ì „ì²´ í´ë“œ ë‹¤ ëŒë ¤ì„œ ì¼ë°˜í™” ì„±ëŠ¥ ëŒì–´ì˜¬ë¦¬ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd5527",
   "metadata": {},
   "source": [
    "## split train data into 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362983b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drivers = sorted(driver_df['subject'].unique())\n",
    "\n",
    "n_folds = 5\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold_splits = []\n",
    "for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(all_drivers)):\n",
    "\ttrain_drivers = [all_drivers[i] for i in train_indices]\n",
    "\tval_drivers = [all_drivers[i] for i in val_indices]\n",
    "\n",
    "\tfold_splits.append({\n",
    "\t\t'fold': fold_idx+1,\n",
    "\t\t'train_drivers': train_drivers,\n",
    "\t\t'val_drivers': val_drivers\n",
    "\t})\n",
    "\n",
    "\tprint(\"Fold\", fold_idx+1)\n",
    "\tprint(\"train:\", train_drivers, \"val:\", val_drivers)\n",
    "\t\n",
    "\ttrain_imgs = driver_df[driver_df['subject'].isin(train_drivers)]\n",
    "\tval_imgs = driver_df[driver_df['subject'].isin(val_drivers)]\n",
    "\tprint(f\"í•™ìŠµ ì´ë¯¸ì§€: {len(train_imgs)}ê°œ\")\n",
    "\tprint(f\"ê²€ì¦ ì´ë¯¸ì§€: {len(val_imgs)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be10ee1",
   "metadata": {},
   "source": [
    "## define DraiverDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriverDataset(Dataset):\n",
    "    \"\"\"ìš´ì „ì í–‰ë™ ë°ì´í„°ì…‹\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, driver_df, driver_list, transform=None, is_test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        if is_test:\n",
    "            test_images_dir = data_dir\n",
    "            for img_name in os.listdir(test_images_dir):\n",
    "                self.images.append(os.path.join(test_images_dir, img_name))\n",
    "        else: #is_train\n",
    "            driver_subset = driver_df[driver_df['subject'].isin(driver_list)]\n",
    "\n",
    "            for _, row in driver_subset.iterrows():\n",
    "                class_name = row['classname']\n",
    "                img_name = row['img']\n",
    "                img_path = os.path.join(data_dir, class_name, img_name)\n",
    "\n",
    "                self.images.append(img_path)\n",
    "                class_idx = int(class_name[1:])\n",
    "                self.labels.append(class_idx)\n",
    "        print(f\"{'í…ŒìŠ¤íŠ¸' if is_test else 'ìš´ì „ì' + str(len(driver_list))+'ëª…'}, ë°ì´í„° {len(self.images)}ê°œ ì´ë¯¸ì§€\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return image, os.path.basename(img_path)\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1464c30",
   "metadata": {},
   "source": [
    "## define online transform\n",
    "ì„œí˜„ë‹˜ ì˜¨ë¼ì¸ ì¦ê°• ì¶”ê°€. clahe ì ìš©í•œ ê±¸ë¡œ ëŒë¦¬ê¸°ì— color jitterë§Œ ì œì™¸í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee806ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_train = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop(img_size, scale=(0.85, 1.0)),\n",
    "#     transforms.RandomRotation(15),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.05),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                          [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.85, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07017e7e",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f4d71",
   "metadata": {},
   "source": [
    "### define custom resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "EMBED_DIM = 512\n",
    "DROPOUT = 0.4\n",
    "DROP_PATH = 0.1\n",
    "EARLY_STOP_PATIENCE = 10 \n",
    "\n",
    "class ResNetDWithHead(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim=512, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            'resnet50d',\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool='',\n",
    "            drop_path_rate=DROP_PATH,\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        in_ch = self.backbone.num_features\n",
    "\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, in_ch, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(in_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(in_ch, embed_dim),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward_features_map(self, x):\n",
    "        return self.backbone.forward_features(x)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x = self.forward_features_map(x)\n",
    "        x = self.embed(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27fbf4f",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca861e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"DataLoader workerì˜ ì¬í˜„ì„± ë³´ì¥\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a03c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(fold_idx, train_drivers, val_drivers):\n",
    "    print(f\"==== Fold {fold_idx}/{n_folds} ====\")\n",
    "\n",
    "    # ===== ë°ì´í„°ì…‹ & ë¡œë” =====\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "\n",
    "    train_dataset = DriverDataset(\n",
    "        train_dir, driver_df, train_drivers,\n",
    "        transform=transform_train, is_test=False\n",
    "    )\n",
    "    val_dataset = DriverDataset(\n",
    "        train_dir, driver_df, val_drivers,\n",
    "        transform=transform_eval, is_test=False\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=True,\n",
    "                              worker_init_fn=worker_init_fn, generator=g)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=True,\n",
    "                            worker_init_fn=worker_init_fn, generator=g)\n",
    "\n",
    "    print(f\"í•™ìŠµ ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")\n",
    "    print(f\"ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}\")\n",
    "\n",
    "    # ===== ëª¨ë¸ =====\n",
    "    model = ResNetDWithHead(\n",
    "        num_classes=num_classes,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        dropout=DROPOUT,\n",
    "    ).to(device)\n",
    "\n",
    "    # # ---- layer1ê¹Œì§€ ë™ê²° (stem~layer1 freeze) ----\n",
    "    # for n, p in model.backbone.named_parameters():\n",
    "    #     if n.startswith('layer2') or n.startswith('layer3') or n.startswith('layer4'):\n",
    "    #         p.requires_grad = True\n",
    "    #     else:\n",
    "    #         p.requires_grad = False\n",
    "    # # headëŠ” í•™ìŠµ\n",
    "    # for p in model.embed.parameters(): p.requires_grad = True\n",
    "    # for p in model.proj.parameters(): p.requires_grad = True\n",
    "    # for p in model.classifier.parameters(): p.requires_grad = True\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° í†µê³„\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  ì „ì²´ íŒŒë¼ë¯¸í„°:      {total_params:>15,}\")\n",
    "    print(f\"  í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°:  {trainable_params:>15,} ({100*trainable_params/total_params:>6.2f}%)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # ===== ì†ì‹¤/ì˜µí‹°ë§ˆ/ìŠ¤ì¼€ì¤„ëŸ¬ =====\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "\n",
    "    # optimizer = optim.AdamW(\n",
    "    #     filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    #     lr=3e-4,\n",
    "    #     weight_decay=5e-2\n",
    "    # )\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=3e-4,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    warmup_epochs = 5\n",
    "    cosine_epochs = max(1, num_epochs - warmup_epochs)\n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[\n",
    "            LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_epochs),\n",
    "            CosineAnnealingLR(optimizer, T_max=cosine_epochs, eta_min=1e-6)\n",
    "        ],\n",
    "        milestones=[warmup_epochs]\n",
    "    )\n",
    "\n",
    "    # ===== ê¸°ë¡ =====\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],\n",
    "               'val_macro_f1': [], 'val_logloss': [], 'learning_rates': []}\n",
    "\n",
    "    # ===== Early Stopping =====\n",
    "    early_stop_patience = EARLY_STOP_PATIENCE\n",
    "    patience_counter = 0\n",
    "    best_metric = float('inf')\n",
    "    best_model_path = f'models/{name}.pth'\n",
    "\n",
    "    print(f\"\\nâ±ï¸ Early Stopping Patience (val logloss ê¸°ì¤€): {early_stop_patience} ep\\n\")\n",
    "    print(\"=\"*70); print(\"ğŸš€ í•™ìŠµ ì‹œì‘\"); print(\"=\"*70)\n",
    "\n",
    "    def eval_on_loader(model, loader, criterion):\n",
    "        model.eval()\n",
    "        total, correct, running_loss = 0, 0, 0.0\n",
    "        all_probs, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(loader, desc='Validating', leave=False)\n",
    "            for images, labels in pbar:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                logits = model(images)\n",
    "\n",
    "                loss = criterion(logits, labels)\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                probs = logits.softmax(dim=1).detach().cpu().numpy()\n",
    "                all_probs.append(probs)\n",
    "                all_labels.append(labels.detach().cpu().numpy())\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}',\n",
    "                                  'acc': f'{100.0*correct/max(1,total):.2f}%'})\n",
    "\n",
    "        avg_loss = running_loss / max(1, total)\n",
    "        acc = 100.0 * correct / max(1, total)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        macro_f1 = f1_score(all_labels, np.argmax(all_probs, axis=1), average='macro')\n",
    "        mlogloss = log_loss(all_labels, all_probs, labels=list(range(num_classes)))\n",
    "        return avg_loss, acc, macro_f1, mlogloss\n",
    "\n",
    "    # ===== ì—í­ ë£¨í”„ =====\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\n{\"=\"*70}')\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'{\"=\"*70}')\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['learning_rates'].append(current_lr)\n",
    "\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        pbar = tqdm(train_loader, desc='Training', leave=False)\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(images)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * labels.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}',\n",
    "                              'acc': f'{100.0*train_correct/max(1,train_total):.2f}%',\n",
    "                              'lr': f'{current_lr:.6f}'})\n",
    "\n",
    "        epoch_train_loss = train_loss / max(1, train_total)\n",
    "        epoch_train_acc = 100.0 * train_correct / max(1, train_total)\n",
    "\n",
    "        # ---- Validate ----\n",
    "        val_loss, val_acc, val_macro_f1, val_logloss = eval_on_loader(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_macro_f1'].append(val_macro_f1)\n",
    "        history['val_logloss'].append(val_logloss)\n",
    "\n",
    "        print(f'\\nğŸ“Š Epoch {epoch+1} ê²°ê³¼:')\n",
    "        print(f'  Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.2f}%')\n",
    "        print(f'  Val   Loss: {val_loss:.4f} | Val Acc:   {val_acc:.2f}%')\n",
    "        print(f'  Val Macro-F1: {val_macro_f1:.4f} | Val LogLoss: {val_logloss:.4f}')\n",
    "        print(f'  LR: {current_lr:.6f}')\n",
    "\n",
    "        # ---- Best / Early Stop ----\n",
    "        if val_logloss < best_metric:\n",
    "            best_metric = val_logloss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'fold': fold_idx,\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'val_macro_f1': val_macro_f1,\n",
    "                'val_logloss': val_logloss,\n",
    "                'model_name': name,\n",
    "                'freeze_mode': 'freeze_to_layer1',\n",
    "                'drop_path_rate': DROP_PATH,\n",
    "                'trainable_params': trainable_params,\n",
    "                'total_params': total_params,\n",
    "                'seed': SEED\n",
    "            }, best_model_path)\n",
    "            print(f'  âœ… ìµœê³  ì„±ëŠ¥(val_logloss) ëª¨ë¸ ì €ì¥! (val_logloss: {val_logloss:.4f})')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  â³ Early Stopping ì¹´ìš´í„°: {patience_counter}/{EARLY_STOP_PATIENCE}')\n",
    "            if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "                print(f'\\n{\"=\"*70}')\n",
    "                print(f'ğŸ›‘ Early Stopping ë°œë™! (Epoch {epoch+1}) â€” val_logloss ê°œì„  ì—†ìŒ')\n",
    "                print(f'   ìµœê³  val_logloss: {best_metric:.4f}')\n",
    "                print(f'   ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {best_model_path}')\n",
    "                print(f'{\"=\"*70}')\n",
    "                break\n",
    "\n",
    "    final_epoch = epoch + 1\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"âœ… Fold {fold_idx} í•™ìŠµ ì™„ë£Œ! (EXP-1+)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  ì´ í•™ìŠµ ì—í­: {final_epoch}/{num_epochs}\")\n",
    "    print(f\"  ìµœê³  Macro-F1: {max(history['val_macro_f1']):.4f}\")\n",
    "    print(f\"  ìµœì € Val Loss: {min(history['val_loss']):.4f}\")\n",
    "    print(f\"  ìµœì € LogLoss:  {min(history['val_logloss']):.4f}\")\n",
    "    print(f\"  ìµœê³  Val Acc:  {max(history['val_acc']):.2f}%\")\n",
    "    print(f\"  ëª¨ë¸ ì €ì¥: {best_model_path}\")\n",
    "    print(f\"  í•™ìŠµ íŒŒë¼ë¯¸í„°: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return {\n",
    "        'fold': fold_idx,\n",
    "        'history': history,\n",
    "        'best_macro_f1': max(history['val_macro_f1']),\n",
    "        'best_val_loss': min(history['val_loss']),\n",
    "        'best_val_logloss': min(history['val_logloss']),\n",
    "        'best_val_acc': max(history['val_acc']),\n",
    "        'model_path': best_model_path,\n",
    "        'stopped_epoch': final_epoch,\n",
    "        'model_name': name,\n",
    "        'freeze_mode': 'freeze_to_layer1',\n",
    "        'trainable_params': trainable_params,\n",
    "        'total_params': total_params,\n",
    "        'seed':SEED\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c9805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Fold 2 í•™ìŠµ (Inception-Cë¶€í„° Fine-tuning) ==========\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "fold_info = fold_splits[0]  # Fold 2\n",
    "fold_idx = fold_info['fold']\n",
    "train_drivers = fold_info['train_drivers']\n",
    "val_drivers = fold_info['val_drivers']\n",
    "\n",
    "\n",
    "print(f\"==== {name} ====\")\n",
    "\n",
    "\n",
    "fold_result = train_fold(\n",
    "    fold_idx, \n",
    "    train_drivers, \n",
    "    val_drivers,\n",
    ")\n",
    "\n",
    "all_fold_results.append(fold_result)\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ğŸ“Š ìµœì¢… ê²°ê³¼\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Fold: {fold_result['fold']}\")\n",
    "print(f\"  ìµœì € Val Loss: {fold_result['best_val_loss']:.4f}\")\n",
    "print(f\"  í•´ë‹¹ Val Acc: {fold_result['best_val_acc']:.2f}%\")\n",
    "print(f\"  í•™ìŠµ ì™„ë£Œ ì—í­: {fold_result['stopped_epoch']}\")\n",
    "print(f\"  Freeze ëª¨ë“œ: {fold_result['freeze_mode']}\")\n",
    "print(f\"  í•™ìŠµ íŒŒë¼ë¯¸í„°: {fold_result['trainable_params']:,} / {fold_result['total_params']:,}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc605bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== í•™ìŠµ ê³¡ì„  ì‹œê°í™” (4ê°œ ê·¸ë˜í”„) ==========\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for result in all_fold_results:\n",
    "    fold_idx = result['fold']\n",
    "    history = result['history']\n",
    "    stopped_epoch = result['stopped_epoch']\n",
    "    best_val_loss = result['best_val_loss']\n",
    "    \n",
    "    # 1. Loss ê·¸ë˜í”„\n",
    "    ax1 = axes[0, 0]\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax1.plot(epochs, history['train_loss'], label='Train Loss', \n",
    "             marker='o', linewidth=2, alpha=0.8, color='#1f77b4')\n",
    "    ax1.plot(epochs, history['val_loss'], label='Val Loss', \n",
    "             marker='s', linewidth=2, alpha=0.8, color='#ff7f0e')\n",
    "    \n",
    "    # ìµœì € Val Loss ì§€ì  í‘œì‹œ\n",
    "    best_epoch = np.argmin(history['val_loss']) + 1\n",
    "    ax1.scatter(best_epoch, best_val_loss, color='red', s=250, zorder=5, \n",
    "                marker='*', edgecolors='black', linewidths=2,\n",
    "                label=f'Best (Epoch {best_epoch})')\n",
    "    \n",
    "    # Early Stopping ì§€ì  í‘œì‹œ\n",
    "    if stopped_epoch < num_epochs:\n",
    "        ax1.axvline(stopped_epoch, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.5, label=f'Early Stop (E{stopped_epoch})')\n",
    "    \n",
    "    ax1.set_title(f'Fold {fold_idx} - Loss (Multiclass Log Loss)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.legend(loc='best', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Accuracy ê·¸ë˜í”„\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(epochs, history['train_acc'], label='Train Acc', \n",
    "             marker='o', linewidth=2, alpha=0.8, color='#2ca02c')\n",
    "    ax2.plot(epochs, history['val_acc'], label='Val Acc', \n",
    "             marker='s', linewidth=2, alpha=0.8, color='#d62728')\n",
    "    \n",
    "    # ìµœê³  Val Acc ì§€ì  í‘œì‹œ\n",
    "    best_acc_epoch = np.argmax(history['val_acc']) + 1\n",
    "    best_val_acc = max(history['val_acc'])\n",
    "    ax2.scatter(best_acc_epoch, best_val_acc, color='green', s=250, zorder=5,\n",
    "                marker='*', edgecolors='black', linewidths=2,\n",
    "                label=f'Best (Epoch {best_acc_epoch})')\n",
    "    \n",
    "    if stopped_epoch < num_epochs:\n",
    "        ax2.axvline(stopped_epoch, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.5, label=f'Early Stop (E{stopped_epoch})')\n",
    "    \n",
    "    ax2.set_title(f'Fold {fold_idx} - Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.legend(loc='best', fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Learning Rate ê·¸ë˜í”„\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(epochs, history['learning_rates'], marker='o', linewidth=2, \n",
    "             color='purple', alpha=0.8, label='Learning Rate')\n",
    "    ax3.set_title(f'Fold {fold_idx} - Learning Rate Schedule', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch', fontsize=12)\n",
    "    ax3.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.legend(loc='best', fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3, which='both')\n",
    "    \n",
    "    if stopped_epoch < num_epochs:\n",
    "        ax3.axvline(stopped_epoch, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.5)\n",
    "    \n",
    "    # 4. Train vs Val ë¹„êµ (Loss & Acc)\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Loss ì°¨ì´\n",
    "    loss_diff = np.array(history['train_loss']) - np.array(history['val_loss'])\n",
    "    ax4_twin = ax4.twinx()\n",
    "    \n",
    "    ax4.plot(epochs, loss_diff, marker='o', linewidth=2, \n",
    "            color='#e377c2', alpha=0.7, label='Loss Diff (Train - Val)')\n",
    "    ax4.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    ax4.set_xlabel('Epoch', fontsize=12)\n",
    "    ax4.set_ylabel('Loss Difference', fontsize=12, color='#e377c2')\n",
    "    ax4.tick_params(axis='y', labelcolor='#e377c2')\n",
    "    \n",
    "    # Accuracy ì°¨ì´\n",
    "    acc_diff = np.array(history['train_acc']) - np.array(history['val_acc'])\n",
    "    ax4_twin.plot(epochs, acc_diff, marker='s', linewidth=2,\n",
    "                 color='#bcbd22', alpha=0.7, label='Acc Diff (Train - Val)')\n",
    "    ax4_twin.set_ylabel('Accuracy Difference (%)', fontsize=12, color='#bcbd22')\n",
    "    ax4_twin.tick_params(axis='y', labelcolor='#bcbd22')\n",
    "    \n",
    "    ax4.set_title(f'Fold {fold_idx} - Overfitting ëª¨ë‹ˆí„°ë§', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Legend í†µí•©\n",
    "    lines1, labels1 = ax4.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax4_twin.get_legend_handles_labels()\n",
    "    ax4.legend(lines1 + lines2, labels1 + labels2, loc='best', fontsize=9)\n",
    "    \n",
    "    if stopped_epoch < num_epochs:\n",
    "        ax4.axvline(stopped_epoch, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs(f'./plots/{name}', exist_ok=True)\n",
    "plt.savefig(f'./plots/{name}/{name}_losscurve_detailed.png', \n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ========== í†µê³„ ì¶œë ¥ ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ğŸ“ˆ {name} í•™ìŠµ í†µê³„ ìƒì„¸\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ì´ˆê¸° Train Loss: {history['train_loss'][0]:.4f}\")\n",
    "print(f\"ìµœì¢… Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"ìµœì € Train Loss: {min(history['train_loss']):.4f} (Epoch {np.argmin(history['train_loss'])+1})\")\n",
    "print(f\"\\nì´ˆê¸° Val Loss: {history['val_loss'][0]:.4f}\")\n",
    "print(f\"ìµœì¢… Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"ìµœì € Val Loss: {min(history['val_loss']):.4f} (Epoch {np.argmin(history['val_loss'])+1})\")\n",
    "print(f\"\\nì´ˆê¸° Train Acc: {history['train_acc'][0]:.2f}%\")\n",
    "print(f\"ìµœì¢… Train Acc: {history['train_acc'][-1]:.2f}%\")\n",
    "print(f\"ìµœê³  Train Acc: {max(history['train_acc']):.2f}% (Epoch {np.argmax(history['train_acc'])+1})\")\n",
    "print(f\"\\nì´ˆê¸° Val Acc: {history['val_acc'][0]:.2f}%\")\n",
    "print(f\"ìµœì¢… Val Acc: {history['val_acc'][-1]:.2f}%\")\n",
    "print(f\"ìµœê³  Val Acc: {max(history['val_acc']):.2f}% (Epoch {np.argmax(history['val_acc'])+1})\")\n",
    "print(f\"\\nì´ˆê¸° LR: {history['learning_rates'][0]:.6f}\")\n",
    "print(f\"ìµœì¢… LR: {history['learning_rates'][-1]:.6f}\")\n",
    "print(f\"LR ë³€ê²½ íšŸìˆ˜: {len(set(history['learning_rates'])) - 1}íšŒ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========== Overfitting ë¶„ì„ ==========\n",
    "final_loss_gap = history['train_loss'][-1] - history['val_loss'][-1]\n",
    "final_acc_gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ” Overfitting ë¶„ì„\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ìµœì¢… Loss ì°¨ì´ (Train - Val): {final_loss_gap:+.4f}\")\n",
    "print(f\"ìµœì¢… Acc ì°¨ì´ (Train - Val): {final_acc_gap:+.2f}%\")\n",
    "\n",
    "if final_acc_gap > 10:\n",
    "    print(\"âš ï¸ ê²½ê³ : ì‹¬ê°í•œ Overfitting ê°ì§€! (Acc ì°¨ì´ > 10%)\")\n",
    "elif final_acc_gap > 5:\n",
    "    print(\"âš ï¸ ì£¼ì˜: ì•½ê°„ì˜ Overfitting ê°ì§€ (Acc ì°¨ì´ > 5%)\")\n",
    "else:\n",
    "    print(\"âœ… ì–‘í˜¸: Overfittingì´ ì˜ ì œì–´ë˜ê³  ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edec7b0",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "driver_df = pd.read_csv(driver_csv_path)\n",
    "\n",
    "# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
    "test_dataset = DriverDataset(\n",
    "    test_dir, driver_df, [], \n",
    "    transform=transform_eval, is_test=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_dataset)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Submission íŒŒì¼ ìƒì„± (í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©) ==========\n",
    "\n",
    "\n",
    "\n",
    "# 2. ëª¨ë¸ ìƒì„±\n",
    "model = ResNetDWithHead(\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "# 3. ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "best_model_path = f'models/{name}.pth'\n",
    "print(f\"\\nğŸ“ ëª¨ë¸ ë¡œë“œ ì¤‘: {best_model_path}...\")\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
    "    print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"   - Fold: {checkpoint.get('fold', 'N/A')}\")\n",
    "    print(f\"   - Epoch: {checkpoint.get('epoch', 'N/A')+1}\")\n",
    "    print(f\"   - Val Loss: {checkpoint.get('val_loss', 'N/A'):.4f}\")\n",
    "    print(f\"   - Val logLoss: {checkpoint.get('val_logloss', 'N/A'):.4f}\")\n",
    "    print(f\"   - Val Acc: {checkpoint.get('val_acc', 'N/A'):.2f}%\")\n",
    "    print(f\"   - Val Macro-F1: {checkpoint.get('val_macro_f1', 'N/A'):.4f}\")\n",
    "    print(f\"   - Val LogLoss: {checkpoint.get('val_logloss', 'N/A'):.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    raise\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 4. ì˜ˆì¸¡ ì‹¤í–‰\n",
    "predictions = []\n",
    "img_names = []\n",
    "\n",
    "print(\"\\nğŸ”® ì˜ˆì¸¡ ì§„í–‰ ì¤‘...\")\n",
    "with torch.no_grad():\n",
    "    for images, filenames in tqdm(test_loader, desc='ì˜ˆì¸¡'):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # 5. í™•ë¥  (Softmax) ê³„ì‚°\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        predictions.append(probs.cpu().numpy())\n",
    "        img_names.extend(filenames)\n",
    "\n",
    "final_predictions = np.vstack(predictions)\n",
    "print(f\"\\nâœ… ì˜ˆì¸¡ ì™„ë£Œ: {final_predictions.shape}\")\n",
    "\n",
    "# 6. Submission íŒŒì¼ ìƒì„±\n",
    "class_cols = [f'c{i}' for i in range(num_classes)]\n",
    "\n",
    "submission_data = {'img': img_names}\n",
    "for i, col in enumerate(class_cols):\n",
    "    submission_data[col] = final_predictions[:, i]\n",
    "\n",
    "submission = pd.DataFrame(submission_data)\n",
    "\n",
    "# 7. íŒŒì¼ ì €ì¥\n",
    "os.makedirs('./submissions', exist_ok=True)\n",
    "submission.to_csv(f\"./submissions/{name}_submission.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"âœ… Submission íŒŒì¼ ìƒì„± ì™„ë£Œ!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸ“ ì €ì¥ ê²½ë¡œ: ./submissions/{name}_submission.csv\")\n",
    "print(f\"ğŸ“Š ì´ ì˜ˆì¸¡ ì´ë¯¸ì§€: {len(submission)}ê°œ\")\n",
    "print(f\"ğŸ“‹ í´ë˜ìŠ¤ë³„ í‰ê·  í™•ë¥ :\")\n",
    "for col in class_cols:\n",
    "    print(f\"   {col}: {submission[col].mean():.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ“‹ Submission ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):\")\n",
    "print(submission.head())\n",
    "\n",
    "print(\"\\nğŸ“‹ Submission ìƒ˜í”Œ (ë§ˆì§€ë§‰ 5ê°œ):\")\n",
    "print(submission.tail())\n",
    "\n",
    "# 8. í™•ë¥  ë¶„í¬ ê²€ì¦ (í•©ì´ 1ì¸ì§€ í™•ì¸)\n",
    "prob_sums = submission[class_cols].sum(axis=1)\n",
    "print(f\"\\nâœ… í™•ë¥  í•© ê²€ì¦: min={prob_sums.min():.6f}, max={prob_sums.max():.6f}, mean={prob_sums.mean():.6f}\")\n",
    "if not np.allclose(prob_sums, 1.0, atol=1e-5):\n",
    "    print(\"âš ï¸ ê²½ê³ : ì¼ë¶€ ìƒ˜í”Œì˜ í™•ë¥  í•©ì´ 1ì´ ì•„ë‹™ë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"âœ… ëª¨ë“  ìƒ˜í”Œì˜ í™•ë¥  í•©ì´ 1ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebf980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
