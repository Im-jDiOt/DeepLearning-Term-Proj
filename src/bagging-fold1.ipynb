{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5048,"databundleVersionId":868335,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"alexnet_005_mixup\nBack to Basics + Mixup Augmentation\n\"\"\"\n\nimport kagglehub\nimport os\n\n# ============================================\n# 0. ë°ì´í„°ì…‹ ë° ì„¤ì •\n# ============================================\n\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom sklearn.model_selection import GroupKFold\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# Seed ê³ ì •\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ê²½ë¡œ ì„¤ì •\ntrain_dir = \"/kaggle/input/state-farm-distracted-driver-detection/imgs/train\"\ntest_dir  = \"/kaggle/input/state-farm-distracted-driver-detection/imgs/test\"\ndriver_csv_path = \"/kaggle/input/state-farm-distracted-driver-detection/driver_imgs_list.csv\"\n\n# í•˜ì´í¼íŒŒë¼ë¯¸í„°\nimg_size = 224\nbatch_size = 32  # Mixupì€ ë°°ì¹˜ê°€ í´ìˆ˜ë¡ ì¢‹ì§€ë§Œ GPU í•œê³„ ê³ ë ¤\nnum_classes = 10\nnum_epochs = 40\nlearning_rate = 1e-4 # Mixup ì‚¬ìš© ì‹œ í•™ìŠµì´ ì¡°ê¸ˆ ë”ë””ë¯€ë¡œ LR ì¡°ì ˆ\nnum_workers = 2\npatience = 8 # Mixupì€ Lossê°€ ì§„ë™í•˜ë¯€ë¡œ patienceë¥¼ ë„‰ë„‰íˆ ì¤Œ\n\n# ============================================\n# 1. Mixup í•¨ìˆ˜ ì •ì˜ (í•µì‹¬)\n# ============================================\ndef mixup_data(x, y, alpha=1.0, use_cuda=True):\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size(0)\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n# ============================================\n# 2. Dataset & Transform\n# ============================================\nclass DriverDataset(Dataset):\n    def __init__(self, img_paths, labels=None, transform=None, is_test=False):\n        self.img_paths = img_paths\n        self.labels = labels\n        self.transform = transform\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        path = self.img_paths[idx]\n        img = Image.open(path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        if self.is_test:\n            return img, os.path.basename(path)\n        return img, self.labels[idx]\n\ndef get_data_lists(df, img_dir):\n    img_paths = []\n    labels = []\n    for _, row in df.iterrows():\n        c_name = row[\"classname\"]\n        path = os.path.join(img_dir, c_name, row[\"img\"])\n        img_paths.append(path)\n        labels.append(int(c_name[1:]))\n    return img_paths, labels\n\n# Augmentation ê°•í™”\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)), # ìŠ¤ì¼€ì¼ ì¡°ì •\n    transforms.RandomHorizontalFlip(p=0.0), # ìš´ì „ì„ ìœ„ì¹˜ ë°”ë€Œë©´ ì•ˆë¨! ì ˆëŒ€ ê¸ˆì§€\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# ============================================\n# 3. ë°ì´í„° ë¡œë“œ (Fold 0)\n# ============================================\ndf = pd.read_csv(driver_csv_path)\n# [ìˆ˜ì • ì½”ë“œ] Fold ë²ˆí˜¸ë¥¼ ë°”ê¿”ê°€ë©° ì‹¤í–‰ ê°€ëŠ¥\nTARGET_FOLD = 1 # <--- ì‹¤í–‰í•  ë•Œë§ˆë‹¤ 0, 1, 2, 3, 4ë¡œ ë³€ê²½!\n\ngkf = GroupKFold(n_splits=5)\nfolds = list(gkf.split(df, df[\"classname\"], df[\"subject\"]))\ntrain_idx, val_idx = folds[TARGET_FOLD] # next() ëŒ€ì‹  ì¸ë±ì‹± ì‚¬ìš©\n\ntrain_df = df.iloc[train_idx]\nval_df = df.iloc[val_idx]\n\ntrain_paths, train_labels = get_data_lists(train_df, train_dir)\nval_paths, val_labels = get_data_lists(val_df, train_dir)\n\ntrain_ds = DriverDataset(train_paths, train_labels, transform=train_transform)\nval_ds = DriverDataset(val_paths, val_labels, transform=val_transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\nval_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n# ============================================\n# 4. ëª¨ë¸ ë° í•™ìŠµ (Mixup ì ìš©)\n# ============================================\n# AlexNet Full Unfreeze (003 ë°©ì‹ ë³µê·€)\nmodel = models.alexnet(weights=\"IMAGENET1K_V1\")\nmodel.classifier[6] = nn.Linear(4096, num_classes)\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n\nbest_loss = 999\nbest_model_path = \"best_alexnet_mixup.pth\"\nes_counter = 0\n\nprint(\"ğŸš€ Training Start with Mixup...\")\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    \n    # Mixup Training Loop\n    for x, y in tqdm(train_loader, desc=f\"Ep {epoch+1} Train\", leave=False):\n        x, y = x.to(device), y.to(device)\n        \n        optimizer.zero_grad()\n        \n        # ğŸ”¥ Mixup ì ìš©\n        inputs, targets_a, targets_b, lam = mixup_data(x, y, alpha=0.4, use_cuda=True)\n        outputs = model(inputs)\n        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n        \n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n        \n    avg_train_loss = total_loss / len(train_ds)\n    \n    # Validation (Mixup ì—†ì´ ì •ìƒ í‰ê°€)\n    model.eval()\n    val_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for x, y in tqdm(val_loader, desc=\"Val\", leave=False):\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            val_loss += loss.item() * x.size(0)\n            correct += (out.argmax(1) == y).sum().item()\n            \n    avg_val_loss = val_loss / len(val_ds)\n    val_acc = 100 * correct / len(val_ds)\n    \n    scheduler.step(avg_val_loss)\n    \n    print(f\"Ep {epoch+1} | Train Loss {avg_train_loss:.4f} (Mixup) | Val Loss {avg_val_loss:.4f} | Acc {val_acc:.2f}%\")\n    \n    if avg_val_loss < best_loss:\n        best_loss = avg_val_loss\n        torch.save(model.state_dict(), best_model_path)\n        es_counter = 0\n        print(f\"ğŸ”¥ Best Model Saved: {best_loss:.4f}\")\n    else:\n        es_counter += 1\n        if es_counter >= patience:\n            print(\"â›” Early Stopping\")\n            break\n\n# ============================================\n# 5. ì˜ˆì¸¡ (FiveCrop TTA ìœ ì§€)\n# ============================================\nprint(\"\\nPredicting with TTA...\")\nmodel.load_state_dict(torch.load(best_model_path))\nmodel.eval()\n\ntta_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.FiveCrop(224),\n    transforms.Lambda(lambda crops: torch.stack([\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(transforms.ToTensor()(crop)) \n        for crop in crops\n    ]))\n])\n\ntest_files = sorted(os.listdir(test_dir))\ntest_paths = [os.path.join(test_dir, f) for f in test_files]\ntest_ds = DriverDataset(test_paths, transform=tta_transform, is_test=True)\ntest_loader = DataLoader(test_ds, batch_size=batch_size//2, shuffle=False, num_workers=num_workers)\n\npreds = []\nnames = []\n\nwith torch.no_grad():\n    for x, fnames in tqdm(test_loader):\n        bs, ncrops, c, h, w = x.size()\n        x = x.view(-1, c, h, w).to(device)\n        out = model(x)\n        out = out.view(bs, ncrops, -1).mean(1)\n        prob = torch.softmax(out, dim=1).cpu().numpy()\n        preds.append(prob)\n        names.extend(fnames)\n\npreds = np.vstack(preds)\nsubmission = pd.DataFrame({\n    \"img\": names,\n    **{f\"c{i}\": preds[:, i] for i in range(10)}\n})\nsubmission.to_csv(\"submission_alexnet_mixup_fold1.csv\", index=False)\n\nfrom IPython.display import FileLink\ndisplay(FileLink('submission_alexnet_mixup_fold1.csv'))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}